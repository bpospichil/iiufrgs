%
% exemplo genérico de uso da classe iiufrgs.cls
% $Id: iiufrgs.tex,v 1.1.1.1 2005/01/18 23:54:42 avila Exp $
%
% This is an example file and is hereby explicitly put in the
% public domain.
%
\documentclass[ecp,tc]{iiufrgs}
% Para usar o modelo, deve-se informar o programa e o tipo de documento.
% Programas :
%   * cic       -- Graduação em Ciência da Computação
%   * ecp       -- Graduação em Ciência da Computação
%   * ppgc      -- Programa de Pós Graduação em Computação
%   * pgmigro   -- Programa de Pós Graduação em Microeletrônica
%   
% Tipos de Documento:
%   * tc                -- Trabalhos de Conclusão (apenas cic e ecp)
%   * diss ou mestrado  -- Dissertações de Mestrado (ppgc e pgmicro)
%   * tese ou doutorado -- Teses de Doutorado (ppgc e pgmicro)
%   * ti                -- Trabalho Individual (ppgc e pgmicro)
% 
% Outras Opções:
%   * english    -- para textos em inglês
%   * openright  -- Força início de capítulos em páginas ímpares (padrão da
%                   biblioteca)
%   * oneside    -- Desliga frente-e-verso
%   * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def


% Use unicode
\usepackage[utf8]{inputenc}   % pacote para acentuação

% Necessário para incluir figuras
\usepackage{graphicx}           % pacote para importar figuras


\usepackage{times}              % pacote para usar fonte Adobe Times

\usepackage{amsmath}
% \usepackage{palatino}
% \usepackage{mathptmx}          % p/ usar fonte Adobe Times nas fórmulas

\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações abnt

\usepackage{graphicx}
\usepackage{caption}% itens individuais da nominata podem ser redefinidos com os comandos
% abaixo:
% \renewcommand{\nominataReit}{Prof\textsuperscript{a}.~Wrana Maria Panizzi}
% \renewcommand{\nominataReitname}{Reitora}
% \renewcommand{\nominataPRE}{Prof.~Jos{\'e} Carlos Ferraz Hennemann}
% \renewcommand{\nominataPREname}{Pr{\'o}-Reitor de Ensino}
% \renewcommand{\nominataPRAPG}{Prof\textsuperscript{a}.~Joc{\'e}lia Grazia}
% \renewcommand{\nominataPRAPGname}{Pr{\'o}-Reitora Adjunta de P{\'o}s-Gradua{\c{c}}{\~a}o}
% \renewcommand{\nominataDir}{Prof.~Philippe Olivier Alexandre Navaux}
% \renewcommand{\nominataDirname}{Diretor do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataCoord}{Prof.~Carlos Alberto Heuser}
% \renewcommand{\nominataCoordname}{Coordenador do PPGC}
% \renewcommand{\nominataBibchefe}{Beatriz Regina Bastos Haro}
% \renewcommand{\nominataBibchefename}{finalBibliotec{\'a}ria-chefe do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataChefeINA}{Prof.~Jos{\'e} Valdeni de Lima}
% \renewcommand{\nominataChefeINAname}{Chefe do \deptINA}
% \renewcommand{\nominataChefeINT}{Prof.~Leila Ribeiro}
% \renewcommand{\nominataChefeINTname}{Chefe do \deptINT}

% A seguir são apresentados comandos específicos para alguns
% tipos de documentos.

% Relatório de Pesquisa [rp]:
% \rp{123}             % numero do rp
% \financ{CNPq, CAPES} % orgaos financiadores

% Trabalho Individual [ti]:
% \ti{123}     % numero do TI
% \ti[II]{456} % no caso de ser o segundo TI

% Monografias de Especialização [espec]:
% \espec{Redes e Sistemas Distribuídos}      % nome do curso
% \coord[Profa.~Dra.]{Weber}{Taisy da Silva} % coordenador do curso
% \dept{INA}     
\usepackage{subcaption}
%	
% Informações gerais
%
\title{Sistema de Baixo Custo para Detecção de Vagas de Estacionamento por Visão Computacional}

\author{Pospichil}{Bruno Meybom}
% alguns documentos podem ter varios autores:
%\author{Flaumann}{Frida Gutenberg}
%\author{Flaumann}{Klaus Gutenberg}

% orientador e co-orientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Jung}{Claudio Rosito}
%\coadvisor[Prof.~Dr.]{Knuth}{Donald Ervin}

% a data deve ser a da defesa; se nao especificada, são gerados
% mes e ano correntes
\date{julho}{2015}

% o local de realização do trabalho pode ser especificado (ex. para TCs)
% com o comando \location:
\location{Porto Alegre}{RS}

%
% palavras-chave
% iniciar todas com letras minúsculas, exceto no caso de abreviaturas
%
\keyword{Visão computacional}
\keyword{Processamento de imagem}
\keyword{Comparação de histogramas}
\keyword{Vagas de estacionamento}
\keyword{Veículos}

%
% inicio do documen\nocite{*}to
%
\begin{document}

% folha de rosto
% às vezes é necessário redefinir algum comando logo antes de produzir
% a folha de rosto:
% \renewcommand{\coordname}{Coordenadora do Curso}
\maketitle

% dedicatoria
\clearpage
\begin{flushright}
\mbox{}\vfill
{\sffamily\itshape
``Não seremos limitados pela informação que temos.\\
Seremos limitados por nossa habilidade de processar esta informação.''\\}
--- \textsc{Peter Drucker}
\end{flushright}

% agradecimentos
\chapter*{Agradecimentos}
Primeiramente, aos meus pais, Elizete e Claudio e ao meu irmão, Ânderson, pelo amor e apoio. O caminho até aqui não foi fácil, tampouco curto, porém com o apoio de vocês se tornou algo atingível. Não tenho dúvidas que se não fosse por de vocês, não estaria aqui neste momento.

Aos meus familiares que, nem sempre próximos, mas com palavras de apoio e com imprescindível contribuição na formação do meu caráter, se hoje posso almejar um belo futuro, em grande parte vocês são os responsáveis.

A UFRGS, pela oportunidade de conviver com alguns dos mais respeitados pesquisadores do Brasil, se hoje chego com portas abertas na maior parte das empresas é pela qualidade do ensino que é ministrado.

Ao meu orientador, Prof. Claudio, pelo suporte e apoio durante a confecção do Trabalho de Graduação, além da compreensão quando os prazos não estavam sendo atendidos rigorosamente.

Aos demais professores, participar desta troca de conhecimentos com vocês foi um privilégio.

Aos meus colegas do Instituto de Pesquisas Eldorado e da HP Brasil, pela parceria e compreensão nos momentos que necessitei me ausentar ou me dedicar a alguma tarefa fora do escopo. Gostaria de estender o agradecimento às demais empresas que eu fiz parte durante a graduação: T\&T, ESOS e CWI.

Ao Instituto de Pesquisas Eldorado, por incentivar e fomentar a pesquisa relacionada a esse projeto. 

Aos meus colegas, que tornaram essa etapa da minha vida muito mais divertida.

E a todos que participaram direta ou indiretamente de todas as etapas que fizeram parte da minha formação, o meu muito obrigado.


% resumo na língua do documento
\begin{abstract}
Este trabalho busca validar algumas técnicas para solucionar o problema da detecção de ocupação de vagas de estacionamento. A solução utiliza visão computacional e tem baixo custo.

Serão apresentadas duas propostas a serem validades: a aplicabilidade em implementar em um hardware minimalista e na capacidade de detecção. 

A primeira vale-se do uso de classificadores Haar e subtração de fundo para identificar alterações na região de uma vaga. Já a segunda, faz uma análise quadro-a-quadro do estacionamento e, utilizando comparação entre histogramas, busca detectar a ocupação da vaga. 

Na implementação do projeto foram utilizadas capturas de um hardware real, um minicomputador Raspberry Pi B de segunda geração e uma câmera sem filtro infravermelho.
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{Low-Cost System to Detect Parking Spaces Using Computer Vision}{Computer vision, image processing, histogram comparison, parking spot, vehicles}
This paper aims to validate some techiniques to solve the parking spot occupancy detection problem, applying a low-cost solution, using computer vision.

Two proposals will be validated considering the applicability to implement on a minimal hardware and the detection capacity.

The first one uses Haar Classifiers and Background Subtraction to identify modifications on the spot area. The second makes a analsys from the parking lot frame-by-frame and using histogram comparison aims to detect the occupancy. 

During the validation the captures used was taken using the real hardware, a mini-computer Raspberry Pi B 2nd generation with a NoIR camera.
\end{englishabstract}

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{SPMD}
        \item[DLT] Direct Linear Transform
        \item[CV] Computer Vision
        \item[RGB] Red, Green and Blue
        \item[BGR] Blue, Green and Red
        \item[BGRA] Blue, Green, Red and Alpha
        \item[CIE] Commission Internationale de L'éclairage
        \item[PMS] Pantone Matching System
        \item[NCS] Natural Color System
        \item[USD] United States Dollar
        \item[MB] Megabytes ($ 10\times10^6 $ bytes)
        \item[GB] Gigabytes ($ 10\times10^9 $ bytes)
        \item[BGS] Background Subtraction
        \item[RCA] Radio Corporation of America
        \item[ROI] Region of Interest
        \item[GMM] Gaussian Mixture Model
        \item[PUCRS] Pontifícia Universidade Católica do Rio Grande do Sul
        \item[HSV] Hue, saturation and value
        \item[HSL] Hue, saturation and lightness
        \item[RPI] Raspberry Pi
        \item[CMOS] Complementary Metal-Oxide Semiconductor
        \item[QXGA] Quantum Extended Graphics Array
        \item[XML] Extensible Markup Language
        \item[UML] Unified Modeling Language
        \item[FPS] Frames per second
    \end{listofabbrv}

% idem para a lista de símbolos
%\begin{listofsymbols}{$\alpha\beta\pi\omega$}
%       \item[$\sum{\frac{a}{b}}$] Somatório do produtório
%       \item[$\alpha\beta\pi\omega$] Fator de inconstância do resultado
%\end{listofsymbols}

% lista de figuras
\listoffigures

% lista de tabelas
\listoftables

% sumario
\tableofcontents

% aqui comeca o texto propriamente dito

% introducao
\chapter{Introdução}
Um dos maiores problemas das metrópoles atualmente é, sem dúvidas, o gerenciamento de uma crescente frota de veículos, que dobrou nos últimos dez anos. Com esse crescimento, se torna cada vez mais complicado encontrar um espaço para deixar seu carro, inclusive em espaços reservados para esse fim - os estacionamentos. 

Por esse motivo, não é incomum encontrarmos sistemas que buscam facilitar a tarefa de encontrar uma vaga livre: em shoppings centers é comum se deparar com sensores individuais, com indicação se a vaga está disponível (verde), ocupada (vermelho) ou se é especial (azul). No entanto, ao migrarmos para estacionamentos descobertos, essa solução não se torna aplicável, devido a sensibilidade dos equipamentos empregados. Inclusive, hoje não encontramos soluções comerciais para tal fim.

Esse trabalho se propõe a validar uma possíveis soluções para implementar um sistema de detecção de vagas de estacionamento utilizando visão computacional, que atualmente é uma das áreas mais promissoras dentro da computação. Além disso, a solução será concebida para ser aplicável em um Raspberry Pi, tornando-a de baixo custo: o conjunto dos equipamentos necessários custa cerca de 80 dólares americanos.

\chapter{Fundamentação Teórica e Trabalhos Relacionados}

\section{Modelos de Câmeras}

\subsection{Câmera Esteonopeica (\textit{pinhole})}
O  modelo  de  câmera  estenopeica  é  o  mais  simples  existente,  proposto  por  Filippo Brunelleschi, no início do século XV, é matematicamente conveniente e, apesar de sua simplicidade, provê uma aproximação aceitável do processo de captura de imagem. Imagine uma caixa, aonde existe um pequeno furo (do grego \textit{stenopo}, por isso estenopeica) no centro de uma das faces e sua face oposta sendo substituída por uma superfície translúcida. Se você colocar essa caixa em uma sala com pouca iluminação, com alguma fonte luminosa em frente à superfície com o furo, a imagem dessa fonte luminosa será exibida invertida na face translúcida, conforme pode ser visto na Figura~\ref{fig:image1}.

\begin{figure}
\centering
\caption{Modelo de câmera estenopeica}
\includegraphics{../../../Dropbox/__TG1/_images/image1}
\label{fig:image1}
\legend{Fonte: \cite{Forsyth2011}}
\end{figure}

Esse modelo define um mapeamento geométrico do mundo 3D para o plano de imagem 2D, conhecido como projeção perspectiva, que cria uma imagem invertida e, as vezes é conveniente se pensar numa imagem virtual, criada em um plano entre a face contendo o furo e a imagem original, à mesma distância entre essa mesma face e o plano da imagem. Conforme o contexto, pode ser útil pensar na imagem virtual ou na real.

A projeção em perspectiva gera alguns efeitos na imagem projetada: o tamanho aparente dos objetos depende da sua distância, os mais distantes parecem menores do que os mais próximos.

\begin{figure}
	\centering
	\caption{Distorção de tamanho}
	\includegraphics{../../../Dropbox/__TG1/_images/image2}
	\label{fig:image2}
	\legend{Fonte: \cite{Forsyth2011}}
\end{figure}

Outro efeito gerado na projeção de imagem é que duas retas paralelas no mundo, isso é, cuja intersecção ocorreria em uma distância infinita, se intersectem no horizonte da imagem. Tais coordenadas da intersecção na imagem são chamados pontos de fuga (\textit{vanishing points}). O conjunto dos pontos de fuga correspondentes a retas paralelas em um plano formam uma reta, conhecida como reta de fuga (\textit{vanishing line}).

\begin{figure}
	\centering
	\caption{Retas de fuga}
	\includegraphics{../../../Dropbox/__TG1/_images/image3}
	\label{fig:image3}
	\legend{Fonte: \cite{Hartley2003}}
\end{figure}

\subsection{Parâmetros de Câmeras}
Definimos como parâmetros de câmera as informações que são responsáveis pelo mapeamento de uma cena do mundo 3D para o plano da imagem 2D e entendemos por calibração de câmera a obtenção desses parâmetros. Os parâmetros são divididos em dois grupos: os \textbf{intrínsecos}, que modelam as características e configurações das lentes, do sensor e a geometria e montagem da câmera, e os \textbf{extrínsecos}, que modelam a pose (posicionamento e orientação da câmera no espaço).

\subsubsection{Parâmetros Intrínsecos}

\begin{figure}
	\centering
	\caption{Modelo de câmera estenopeica}
	\includegraphics{../../../Dropbox/__TG1/_images/image4}
	\label{fig:image4}
	\legend{Fonte: \cite{Hartley2003}}
\end{figure}

Consideremos o modelo representado na Figura~\ref{fig:image4}, aonde \texttt{C} representa o centro de projeção, também conhecido como centro da câmera ou centro óptico. Vamos definir um sistema de coordenadas onde \texttt{C} seja a origem, e consideraremos o plano \texttt{Z = f}, que é conhecido como plano da imagem ou plano focal e está posicionado em frente ao centro de projeção e \texttt{f} é a distância focal. A linha que passa pelo centro da câmera e é perpendicular ao plano da imagem é chamada de eixo principal ou eixo óptico e o ponto onde esse eixo encontra o plano da imagem é conhecido como ponto principal (\texttt{p}).

A partir da Figura~\ref{fig:image4}, temos que um ponto no espaço de coordenadas $(X, Y, Z)^T$ é mapeado, por semelhança de triângulos, para um ponto no plano da imagem $ (f\dfrac{X}{Z}, f\dfrac{Y}{Z}) $. Portanto, temos que $ (X, Y, Z)^T \mapsto (f\dfrac{X}{Z}, f\dfrac{Y}{Z})^T $ descreve  a  projeção  de  pontos  do  mundo  para  coordenadas  da  imagem.  Isso  é, mapeando de um espaço 3D para um 2D.

Se o mundo e os pontos da imagem são representados por vetores homogêneos, então, a projeção é facilmente expressada como uma relação linear de suas coordenadas homogêneas. A equação anterior pode ser escrita como uma multiplicação matricial:

\[\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \mapsto \begin{pmatrix} fX \\ fY \\ fZ \end{pmatrix} = \begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \]

Na matriz acima, assumimos que a origem das coordenadas no plano da imagem é o ponto principal. No entanto, isso pode não ser verdade, por isso, podemos considerar que $ (X, Y, Z)^T \mapsto (f\dfrac{X}{Z} + p_x, f\dfrac{Y}{Z} + p_y)^T $ aonde $ (p_x, p_y)^T $ são as coordenadas do ponto principal, conforme a figura~\ref{fig:image5}.

\begin{figure}
	\centering
	\caption{Sistema de coordenadas da imagem e da câmera}
	\includegraphics{../../../Dropbox/__TG1/_images/image5}
	\label{fig:image5}
	\legend{Fonte: \cite{Hartley2003}}
\end{figure}

Assim, podemos escrever a equação em coordenadas homogêneas, chegando a
\[\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \mapsto \begin{pmatrix} fX+Zp_x \\ fY+Zp_y \\ fZ \end{pmatrix} = \begin{bmatrix} f & 0  & p_x & 0 \\ 0 & f & p_y & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \].
Aonde definimos
\[K = \begin{bmatrix} f & 0  & p_x  \\ 0 & f & p_y \\ 0 & 0 & 1\end{bmatrix}\]
Ou, na forma concisa: $ x = K[I|0]x_{cam}$.

A matriz $ K $ é conhecida como \textit{matriz de calibração de câmera}. Na forma concisa, assumimos que a câmera estará posicionada na origem do sistema de coordenadas, com o eixo principal apontando para fora do eixo $ x $ e o ponto $ x_{cam} $ é definido nesse sistema de coordenadas.

\subsubsection{Parâmetros Extrínsecos}
\label{sec:extrinsic}
Os parâmetros extrínsecos relacionam o sistema de coordenadas da câmera com o do mundo. Os dois sistemas de coordenadas se relacionam por rotação e translação, conforme pode ser visto na figura~\ref{fig:image6}

\begin{figure}
	\centering
	\caption{Transformação entre o sistema de coordenadas da câmera e do mundo}
	\includegraphics{../../../Dropbox/__TG1/_images/image6}
	\label{fig:image6}
	\legend{Fonte: \cite{Hartley2003}}
\end{figure}

Sendo $ \widetilde{X} $ um vetor não homogêneo representando um ponto no sistema de coordenadas do mundo e $ \widetilde{X}_{cam} $ representando o mesmo ponto no sistema de coordenadas da câmera, podemos escrever $ \widetilde{X}_{cam} = R(\widetilde{X} - \widetilde{C}) $ , aonde $ \widetilde{C} $ representa as coordenadas do centro da câmera no sistema de coordenadas do mundo e R é uma matriz $ 3 \times 3 $ representando a orientação do sistema de coordenadas da câmera, ou seja:
\[ X_{cam} =
\begin{bmatrix} R & -R\widetilde{C} \\ 
0 & 1 \end{bmatrix}
\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} =
\begin{bmatrix} R & -R\widetilde{C} \\ 
0 & 1 \end{bmatrix}
\textbf{X}  \]

Que, por sua vez, unindo com a matriz de calibração da câmera, temos que $ x = KR[I|-\widetilde{C}]\textbf{X} $ onde $ X $ é dado no sistema de coordenadas do mundo.

Vemos que em uma câmera estenopeica $ P = KR[I|-\widetilde{C}] $ temos nove graus de liberdade: três para $ K(f, p_x, p_y) $, três para $ R $ e três para $ \widetilde{C} $. Os parâmetros de $K$ são conhecidos como \textit{parâmetros internos da câmera}, os
parâmetros $R$ e $\widetilde{C}$ relacionam a orientação da câmera e sua posição no sistema de coordenadas do mundo e são chamados \textit{parâmetros externos}.

Alem disso, pode ser útil deixar o centro da câmera como variável implícita. representando a transformação do sistema de coordenadas do mundo para a imagem como $ X_{cam} = R\widetilde{X} + t $. Nesse caso, a matriz da câmera se torna $ x = K[R|t] $, aonde $ t = -R\widetilde{C} $.

\subsubsection{Imperfeições de Lentes}

Os parâmetros intrínsecos e extrínsecos descritos anteriormente fornecem a descrição do processo de formação de imagem em uma câmera estenopeica, no entanto, ao lidarmos com câmeras de lente reais, encontramos algumas distorções que não foram previstas
até aqui. Considerando que estas são geradas pelo processo construtivo da lente, podemos considerá-las como um tipo específico de parâmetros intrínsecos.

De acordo com \citen{Hartley2003}, a modelagem exata das lentes é uma tarefa complexa, sendo que dessas imperfeições, a distorção radial (ou distorção barril) é a mais relevante em ser corrigida. Essa distorção provoca que uma reta no sistema de coordenadas do mundo seja projetada com uma curvatura no plano da imagem.

\begin{figure}
	\centering
	\caption{Distorção radial}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/image7a}
		\caption{Mapa de pixeis sem distorção}
		\label{fig:image7a}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/image7b}
		\caption{Imagem com distorção radial}
		\label{fig:image7b}
	\end{subfigure}
	\label{fig:image7}
	\legend{Fonte: \cite{Morvan2009}}
\end{figure}

De acordo com \citen{Morvan2009}, a relação entre a posição dos pixeis na imagem distorcida $ (x_d, y_d)^T $ e a imagem corrigida $ (x_u, y_u)^T $ é definida por
\[ \begin{bmatrix}x_u - p_x \\ y_u - p_y \end{bmatrix} = L(r_d)\begin{bmatrix}x_d - p_x \\ y_d - p_y \end{bmatrix}\]
onde $ (p_x, p_y)^T $ são as coordenadas do ponto principal e $ L(r_d) = 1+K_1{r_d}^2 $ em que $ k_1 $ é a quantidade de distorção radial presente na imagem e $ {r_d}^2 = (x_d - p_x)^2 + (y_d - p_x)^2 $.

A correção da distorção radial passa por definirmos os parâmetros $ k_1 $ e $ (p_x, p_y)^T $. Podemos estimar esses valores pelo cálculo da curvatura de uma linha na imagem 2D que corresponde a uma reta no espaço de coordenadas do mundo (3D). Normalmente se utiliza um padrão de calibração (muitas vezes um tabuleiro de xadrez), que consiste de uma sequência de retas conhecidas em coordenadas do mundo e, por meio dos valores obtidos em coordenadas da imagem, é possível estimar sua distorção.

\subsection{Calibração de Câmera}


O processo de calibração de câmera consiste em se estabelecer os parâmetros intrínsecos e extrínsecos que caracterizam a projeção da imagem, isso é, uma câmera é dita calibrada quando o mapeamento entre as coordenadas da imagem e as direções  relativas ao centro da câmera são conhecidas.

Isso nos conduz a definir uma série de valores (nesse caso, a matriz de calibração da câmera) que efetuam o mapeamento de um ponto no sistema de coordenadas do mundo real com um ponto na imagem, um dos algoritmos mais utilizados para fazer essa correspondência de pontos, é a Transformação Linear Direta ou DLT.

\subsubsection{Transformação Linear Direta}

No final da seção~\ref{sec:extrinsic} definimos a forma canônica da matriz de calibração de uma câmera estenopeica, se considerando apenas os parâmetros intrínsecos e extrínsecos (repare que a distorção ocasionada por imperfeições na lente utilizada não está sendo abordada nessa equação), que é $ P = KR[I|-\widetilde{C}] $ e que, por sua vez, admite como solução:

\[
P = \begin{bmatrix} f & & p_x \\ & f & p_y \\ & & 1 \end{bmatrix} [I|0] 
\begin{bmatrix} R & -R\widetilde{C} \\ 0 & 1 \end{bmatrix} = 
\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\ m_{21} & m_{22} & m_{23} & m_{24} \\ m_{31} & m_{32} & m_{33} & m_{34} \end{bmatrix}
\]

Em função disso as coordenadas da imagem e do mundo se relacionam por meio da matriz $ M $, ou seja,

\[
\begin{pmatrix} u_s \\ v_s \\ s \end{pmatrix} = \begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\ m_{21} & m_{22} & m_{23} & m_{24} \\ m_{31} & m_{32} & m_{33} & m_{34} \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix}
\]

Onde $ \begin{pmatrix} X & Y & Z \end{pmatrix}^T $ representam coordenadas do mundo e $ \begin{pmatrix} u & v \end{pmatrix}^T $ coordenadas na imagem (pixeis).

\[
u = \dfrac{m_{11}X + m{12}Y + m_{13}Z + m_{14}}{m_{31}X + m{32}Y + m_{33}Z + m_{34}}
\]
\[
v = \dfrac{m_{21}X + m{22}Y + m_{23}Z + m_{24}}{m_{31}X + m{32}Y + m_{33}Z + m_{34}}
\]

Assim, para cada conjunto de valores $ (u, v, X, Y, Z) $ obtemos duas equações, considerando que existem doze incógnitas na matriz, precisaremos de seus conjuntos de valores, gerando doze equações, assumindo que elas são linearmente independentes),
chegando ao sistema de equações $ 12\times12 $.

\setcounter{MaxMatrixCols}{20}

\[
\begin{bmatrix}
X_1 & Y_1 & Z_1 & 1 & 0 & 0 & 0 & 0 & -u_1X_1 & -u_1Y_1 & -u_1Z_1 & -u_1 \\
0 & 0 & 0 & 0 & X_1 & Y_1 & Z_1 & 1 & -v_1X_1 & -v_1Y_1 & -v_1Z_1 & -v_1 \\
X_2 & Y_2 & Z_2 & 1 & 0 & 0 & 0 & 0 & -u_2X_2 & -u_2Y_2 & -u_2Z_2 & -u_2 \\
0 & 0 & 0 & 0 & X_2 & Y_2 & Z_2 & 1 & -v_2X_2 & -v_2Y_2 & -v_2Z_2 & -v_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
X_n & Y_n & Z_n & 1 & 0 & 0 & 0 & 0 & -u_nX_n & -u_nY_n & -u_nZ_n & -u_n \\
0 & 0 & 0 & 0 & X_n & Y_n & Z_n & 1 & -u_nX_n & -u_nY_n & -u_nZ_n & -v_n 
\end{bmatrix}
\begin{pmatrix}m_{11} \\ m_{12} \\ m_{13} \\ m_{14} \\ m_{21} \\ m_{22} \\ m_{23} \\ m_{24} \\ m_{31} \\ m_{32} \\ m_{33} \\ m_{34} \end{pmatrix} = 0
\]

Uma das características desse sistema é que múltiplos da matriz geram a mesma projeção, o que origina soluções diferentes da trivial no sistema acima. De fato, a solução trivial não é de interesse, pois gera uma matriz de projeção nula. Para obtermos uma solução diferente da trivial, é comum definirmos o valor de um dos parâmetros da matriz de projeção, comumente $ m_{34} = 1 $, fazendo com que o sistema se torne

\[
\begin{bmatrix}
X_1 & Y_1 & Z_1 & 1 & 0 & 0 & 0 & 0 & -u_1X_1 & -u_1Y_1 & -u_1Z_1 \\
0 & 0 & 0 & 0 & X_1 & Y_1 & Z_1 & 1 & -v_1X_1 & -v_1Y_1 & -v_1Z_1 \\
X_2 & Y_2 & Z_2 & 1 & 0 & 0 & 0 & 0 & -u_2X_2 & -u_2Y_2 & -u_2Z_2 \\
0 & 0 & 0 & 0 & X_2 & Y_2 & Z_2 & 1 & -v_2X_2 & -v_2Y_2 & -v_2Z_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
X_n & Y_n & Z_n & 1 & 0 & 0 & 0 & 0 & -u_nX_n & -u_nY_n & -u_nZ_n \\
0 & 0 & 0 & 0 & X_n & Y_n & Z_n & 1 & -u_nX_n & -u_nY_n & -u_nZ_n 
\end{bmatrix}
\begin{pmatrix}m_{11} \\ m_{12} \\ m_{13} \\ m_{14} \\ m_{21} \\ m_{22} \\ m_{23} \\ m_{24} \\ m_{31} \\ m_{32} \\ m_{33} \\ m_{34} \end{pmatrix} = \begin{pmatrix} u_1 \\ v_1 \\ u_2 \\ v_2 \\ \vdots \\ u_n \\ v_n \end{pmatrix}
\]

Assim, obtemos um sistema com 11 incógnitas do tipo $ \vec{A_m} = \vec{b} $ , que é sobre determinado, isso é, a matriz possui dimensão $ m \times n $ e $ m > n $ (nesse caso, $ m = 12 $ e $ n = 11$ ). Este tipo de sistema normalmente não possui uma solução exata, então desejamos buscar uma solução aproximada, que, por sua vez, pode ser encontrada utilizando-se o método dos mínimos quadrados. De fato, é comum usarmos mais do que 12 equações, pois quanto mais valores $ (u, v, X, Y, Z) $ forem obtidos, maior o número de equações utilizadas e mais diluído será o erro encontrado.

\subsubsection{Padrões de Calibração}
\label{calibpat}

Como pudemos ver, o processo de calibração de câmera, principalmente ao se considerar a distorção das lentes, não é um processo dos mais simples de ser executado. Uma alternativa muito eficiente é a utilização de padrões de calibração.

Um padrão de calibração consiste na utilização de uma figura que possua bordas bem definidas, de modo que essas sejam facilmente detectáveis, usualmente utilizamos algo semelhante a um tabuleiro de xadrez, pois suas formas proporcionam essa detecção, como pode ser visto na figura~\ref{fig:calib}. Com esse objeto em mãos, podemos utilizar softwares para obter os parâmetros de calibração da câmera automaticamente, isso é, definir a matriz de calibração da câmera. Além disso, outro ponto importante é que a calibração radial é \textit{off-line}, isso é, pode ser efetuada com a câmera fora de sua posição final, pois não se alteram em caso de alteração da cena. Isso acaba sendo conveniente, pois poderemos instalar uma câmera já calibrada, ficando necessária somente a definição dos parâmetros relativos a cena (extrínsecos).

\begin{figure}
	\centering
	\caption{Exemplo de calibração de câmera}
	\includegraphics{../../../Dropbox/__TG1/_images/calib}
	\label{fig:calib}
	\legend{Fonte: OpenCV Tutorials}
\end{figure}

Uma das alternativas disponíveis é o \textit{Camera Calibration Toolbox for Matlab}, que é um \textit{framework} desenvolvido para ser executado no Matlab com diversos recursos de calibração de câmeras. Além disso, seu código já foi portado para a linguagem C, de modo que foi embutido no OpenCV, que é uma biblioteca para o desenvolvimento de aplicativos na área de visão computacional, originalmente desenvolvida pela Intel.

\subsection{Vista de pássaro}
\label{birds}

Vista de pássaro ou \textit{bird's eye view} consiste na visualização de um objeto ou plano como se estivesse acima do mesmo. Usualmente se efetua matriz de homografia para efetuar a transformação para esse ponto de vista.

\begin{figure}
	\centering
	\caption{Vista de pássaro (\textit{bird's eye view})}
	\begin{subfigure}[b]{0.47\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/birds1}
		\caption{Visão de uma superfície plana}
		\label{fig:birds1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/birds2}
		\caption{Vista de pássaro}
		\label{fig:birds2}
	\end{subfigure}
	\label{fig:image7}
	\legend{Fonte: \cite{Bradski2008}}
\end{figure}


Apesar de não ter utilizado essa terminologia, o processo de calibração de câmera consiste na obtenção de uma ou mais matrizes de homografia que efetuam a correção das distorções provocadas pelas lentes na cena. No caso da vista de pássaro, seria necessário, apenas, encontrar a transformação que mapeasse pontos coplanares para uma mesma perspectiva, o método utilizado é o mesmo para a calibração da câmera: DLT.


\section{Espaços de cores}

Cor é o resultado da percepção da luz que incide na retina em células fotorreceptoras, conhecidas como cones. De acordo com \citen{Shapiro2001}, a percepção de cor depende, além das características físicas da luz, de um processamento complexo pelo olho e cérebro que integra as propriedades do estímulo com a sensibilidade do observador.

\citen{Feitosa-Santana2006}, relatou que existem registros de estudos para encontrar uma maneira de se organizar as cores desde a antiguidade. Dentre os primeiros registros de espaço de cores, pode-se destacar  o trabalho de Pitágoras (circa 570-500 a.C.), que criou um espaço de cores semicircular relacionando as notas da escala musical de tons e meio tons aos planetas que, por sua vez, eram representados por determinadas cores, como pode ser visto na figura~\ref{fig:pit}.

\begin{figure}
	\centering
	\caption{Modelo de espaço de cores de Pitágoras}
	\includegraphics{../../../Dropbox/__TG1/_images/pit}
	\label{fig:pit}
	\legend{Fonte: http://www.colorsystem.com/}
\end{figure}

A colorimetria se tornou tecnicamente útil quando, em 1931, o CIE padronizou as funções de correspondência de cores. determinadas pela sistematização das misturas de cores necessárias para um observador, em condições específicas de iluminação. A partir do experimento executado em 1931 foram definidos os espaços de cores CIE XYZ e RGB.

Hoje em dia existem uma série de espaços de cores que atendem a determinados interesses, alguns com finalidade comercial, outro de uso genérico. Os espaços definidos pelo CIE em 1931 são de uso genérico, aonde ainda podemos adicionar outros como o HSV, CIELAB, CIELuv. Dentre os espaços de cores de fins comerciais, os mais conhecidos são o Pantone (PMS) e o NMS, desenvolvido pelo Scandinavian Colour Institute.

No escopo desse trabalho, as imagens capturadas pelas câmeras utilizam o espaço de cores RGB (na verdade BGR, devido à especificação do OpenCV), contudo, se faz útil efetuar a conversão para outro espaço de cores aonde a iluminação (luminância) seja separada das informações de cores (crominância), cujos modelos mais utilizados são HSV, HSL, CIEXYZ, YCbCr, CIELAB, CIELUV, YUV. \citen{Lee2005} efetuou uma avaliação de similaridade de imagem por meio de intersecção de histogramas, uma técnica que será utilizada nesse trabalho e em seus resultados encontrou que o CIELAB obteve os melhores índices de acerto, por esse motivo, este será o espaço de cores utilizado.

\subsection{Modelo de Cores CIE 1931 RGB}

O modelo de cores aditivo CIE RGB, foi definido a partir dos experimentos executados em 1931 no CIE e foi constituído a partir da teoria de visão colorida tricromática, de Young-Helmholtz e no triângulo de cores de James Clerk Maxwell, representado na figura~\ref{fig:triangle}. Esse modelo acabou se tornando padrão \textit{de facto} devido a ser visualmente intuitivo, uma vez que a maior parte das cores pode ser formada a partir de uma combinação das três cores fundamentais, e esse modelo passou a ser utilizado em equipamentos comerciais em meados da década de 50, quando a RCA introduziu o modelo em seu televisor CT-100, o primeiro colorido da marca.

\begin{figure}
	\centering
	\caption{Triângulo de cores de Maxwell}
	\includegraphics{../../../Dropbox/__TG1/_images/triangle}
	\label{fig:triangle}
	\legend{Fonte: Appalachian State University - Psychology 3215}
\end{figure}

Apesar de ser amplamente difundido, muitas vezes quando estamos efetuando processamento das imagens, esse modelo de cor acaba não sendo a melhor escolha, principalmente pelo fato de que as variações de cor não serem uniformes. Além disso, por meio unicamente de adições não é possível se representar todas as variações de cores, sendo necessários valores negativos para R e G para se atingir toda a gama existente.

\subsection{Modelo de Cores CIE 1976 L*a*b*}

A percepção de pequenas variações entre cores é de suma importância em experimentos de percepção e especificação de cores. Um sistema de cores deve permitir que sejam representadas todas as cores com uma alta precisão. Em processamento de imagens é interessante que em um espaço de cores uma pequena perturbação em uma componente é aproximadamente igualmente perceptível em toda a gama do referido valor, esses modelos de cores são conhecidos como perceptualmente uniformes.

Em 1976 o CIE propôs o modelo L*a*b* (Lab ou CIELAB), que, por definição é perceptualmente uniforme, que é sua maior vantagem se comparado com o RGB de 1931, contudo, não é a única vantagem, com o CIELAB é possível representar toda a a gama de cores, além disso, esse sistema se aproxima bastante do da visão humana (\textit{human vision system}).

Diferentemente do RGB, aonde cada canal representa uma cor básica, no CIELAB temos dois tipos distintos de canais, o canal de luminância (iluminação) $ L* $ e os de crominância $a*$ e $b*$, aonde $a*$ representa as cores oponentes vermelha e verde e $b*$ amarelo e azul: valores positivos de $a$" indicam vermelho e negativos verde, já valores positivo de $b*$ indicam amarelo e negativos azul, como pode ser visto na figura~\ref{fig:cielab}.

\begin{figure}
	\centering
	\caption{Modelo do CIELAB}
	\includegraphics{../../../Dropbox/__TG1/_images/cielab}
	\label{fig:cielab}
	\legend{Fonte: JISC Digital Media}
\end{figure}

\section{Comparação de Histogramas}
\label{ch:hist}
Histograma, também conhecido como distribuição de intensidade, é uma representação gráfica distribuída em colunas de dados, a partir da visualização de um histograma é possível aferir a maneira de distribuição, simetria e dispersão dos dados. Em processamento de imagem, histogramas são usados para se observar as características de uma imagem. Outro ponto extremamente relevante que a comparação de histogramas entre duas imagens é uma das maneiras mais eficientes de se detectar a similaridade entre elas.

\citen{Liu2015}, faz o uso de comparação de histogramas para identificar similaridade em regiões de vídeo, primeiramente a região de interesse é dividida em segmentos (\textit{bins}), e o valor de cada classe é computado:

\[
bin(k) = \sum_{i=0}^{M}\sum_{j=0}^{N}f_k(i,j),(0 \le k \le 255)
\]

\begin{equation}
f_k(i,j)=\begin{cases}
1, & \text{se $p(i,j) = k$}.\\
0, & \text{se $p(i,j) \neq k$}.
\end{cases}
\end{equation}

Aonde $ p(i,j) $ significa o valor do pixel $ i, j$ em determinado quadro, e $ k $ o valor desse pixel em determinado canal. Por simplicidade, iremos considerar o histograma em uma dimensão somente, contudo, é usual se utilizar histogramas em mais de uma dimensão.

Uma vez tendo o histograma criado, existem diversos métodos para se encontrar a distância (ou diferença) entre dois deles, \citen{Liu2015}, descreve três das mais utilizadas atualmente: correlação, chi-quadrado e intersecção, sendo que cada uma utiliza de uma equação característica:

\[
d_{correl}(H_1,H_2) =  \frac{\sum_I (H_1(I) - \bar{H_1}) (H_2(I) - \bar{H_2})}{\sqrt{\sum_I(H_1(I) - \bar{H_1})^2 \sum_I(H_2(I) - \bar{H_2})^2}}
\]
\[
\bar{H_k} =  \frac{1}{N} \sum _J H_k(J)
\]
\[
d_{chi-quad}(H_1,H_2) =  \sum _I  \frac{\left(H_1(I)-H_2(I)\right)^2}{H_1(I)}
\]
\[
d_{intersec}(H_1,H_2) =  \sum _I  \min (H_1(I), H_2(I))
\]

No caso da correlação, quanto maior o valor encontrado, maior a similaridade entre as duas imagens, elas forem iguais, o resultado será 1, assim como na intersecção, valores menores do que 1 indicam diferenças na imagem. Para chi-quadrado, teremos no caso de imagens idênticas 0 e valores maiores do que esse indicam que existem diferenças entre elas. Na documentação do OpenCV encontramos um exemplo que demonstra os valores encontrados na comparação de histogramas, utilizando as figuras~\ref{fig:hist_0}, ~\ref{fig:hist_1} e ~\ref{fig:hist_2}, o resultado obtido está disposto na tabela~\ref{tbl:hist}.

\begin{figure}[h]
	\centering
	\caption{Figuras de exemplo para comparação de histogramas}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_0}
		\caption{Base}
		\label{fig:hist_0}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_1}
		\caption{Teste 1}
		\label{fig:hist_1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_2}
		\caption{Teste 2}
		\label{fig:hist_2}
	\end{subfigure}
	\label{fig:hist}
	\legend{Fonte: OpenCV}
\end{figure}

\begin{table}[h]
	\caption{Resultados comparação de histogramas}
	\begin{center}
		\begin{tabular}{c|c|c|c}
			\textit{Método}  &   \textit{Base x Base}  &   \textit{Base x Teste 1} & \textit{Base x Teste 2}  \\
			\hline
			\hline
			Correlação & 1,000000 &	0,182073 & 0,120447 \\
			\hline
			Chi-quadrado & 0,000000 & 21,184536 & 49,273437 \\
			\hline
			Intersecção & 24,391548 & 3,889029 & 5,775088 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: OpenCV}
	\label{tbl:hist}
\end{table}


\section{Classificadores Haar}
\label{haar}
Classificadores Haar (\textit{Haar Classifiers}), é um framework de detecção de objetos rápido e robusto, proposto por \citen{Viola2004}, esse classificador usa o conceito de \textit{feature}, que é uma parte de informação que é relevante para resolver determinado problema. \textit{Features} podem ser estruturas como bordas, pontos ou objetos. No caso em questão, temos o conceito de perfis Haar (\textit{Haar-like features}), que são padrões geométricos usados nas sub-janelas de detecção, consistem de imagens adjacentes e foram assim chamadas por sua semelhança com as ondulações Haar (\textit{Haar wavelets}).

\begin{figure}
	\centering
	\caption{Exemplos de perfis Haar}
	\includegraphics{../../../Dropbox/__TG1/_images/haar}
	\label{fig:haar}
	\legend{Fonte: \cite{Viola2004}}
\end{figure}

O valor de um perfil Haar é dado pela soma dos pixeis da(s) área(s) sombreada(s), subtraído da soma dos pixeis da(s) área(s) branca(s). Considerando que
as formas são retangulares, seus valores podem ser computados rapidamente utilizando uma representação de imagem conhecida como \textbf{Imagem Integral}.
A imagem integral no pixel x, y contém a soma dos pixeis acima e à esquerda de x, y, inclusive:
\[ ii(x,y) = \sum_{x'\le x,y' \le y} i(x', y') \]

Calculado o valor de cada perfil Haar, um classificador fraco é definido por:
\begin{equation}
	h(x)=\begin{cases}
		1, & \text{se $pf(x) < p\theta$}.\\
		0, & \text{senão}.
	\end{cases}
\end{equation}

Aonde $ x $ é uma sub-janela, $ \theta $ é um limiar e $ p $ define a paridade, indicando a direção do sinal da inequação.

Além do uso de perfis Haar, classificadores Haar utilizam AdaBoost (\textit{adaptive boosting} ou estímulo adaptável), que é um algoritmo de aprendizado de máquina proposto por \citen{Freund1997} que tem como objetivo aumentar a performance, construindo um classificador forte utilizando um conjunto de treinamento e um algoritmo de aprendizado fraco.

A utilização de Classificadores Haar depende da existência de um conjunto de treinamento que tenha um bom desempenho na cena que está sendo capturada. Um uso muito comum desses algoritmos é no reconhecimento de face, uma vez que normalmente esse tipo de captura está sob mesma perspectiva, seu uso em imagens de diferentes perspectivas pode vir a introduzir uma baixa taxa de acerto.

\section{Subtração de Fundo}
Subtração de fundo (ou \textit{background subtraction}, ou \textit{foreground detection}) é um passo fundamental em muitas aplicações no campo de visão computacional e processamento digital de imagens. Consiste, em geral, na comparação de uma imagem com outra imagem que modela o plano de fundo \textit{background}.  A diferença observada consiste de objetos no primeiro plano do vídeo (\textit{foreground}). Fazem parte do primeiro plano todos os objetos que não estão fixos em uma cena. Portanto, podemos definir em linhas gerais a função de detecção como: 
\[ |quadro_i - fundo| > limiar \]

Ao analisar a função, verificamos que existe um limiar (\textit{threshold}) a ser definido  que irá determinar se o pixel em questão faz parte do plano de fundo ou do primeiro plano. Se o valor for mal escolhido teremos resultados insatisfatórios: se o limiar definido for muito alto, poderão ser ignoradas algumas alterações relevantes no quadro e, se for muito baixo, fará com que sejam considerados ruídos desprezíveis.

\begin{figure}
	\centering
	\caption{Definição do \textit{threshold} para \textit{background subtraction}}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs1}
		\caption{Imagem original}
		\label{fig:bgs1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs2}
		\caption{Diferença absoluta}
		\label{fig:bgs2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs3}
		\caption{Limiar muito alto}
		\label{fig:bgs3}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs4}
		\caption{Limiar muito baixo}
		\label{fig:bgs4}
	\end{subfigure}
	\label{fig:imagebgs}
	\legend{Fonte: \cite{Piccardi2004}}
\end{figure}

Atualmente contamos com dezenas de métodos disponíveis para executar o processo de subtração de fundo, que podem ser divididos em diversos grupos em função dos algoritmos utilizados:
\begin{itemize}
\item Básicos: baseados em média e variância ao longo do tempo;
\item Lógica fuzzy;
\item Gaussianas simples;
\item Múltiplas gaussianas
\item Entre outros...
\end{itemize}


\citen{Sobral2013} descreveu o desenvolvimento de um \textit{framework} baseado em OpenCV que proporciona a inclusão de subtração de fundo de maneira extremamente fácil, a \textit{BGSLibrary}, a partir dessa biblioteca, foram selecionados alguns algoritmos para que fosse avaliada, num primeiro momento, a quantidade de ruído encontrada nas imagens de \textit{foreground}. Os métodos selecionados são descritos a seguir.

\subsection{Diferença Entre Quadros}

É um dos métodos mais simples de subtração de fundo (talvez o mais simples), nesse algoritmo, temos como o fundo sendo o valor do quadro anterior, ou seja:
\[|quadro_i - quadro_{i-1} | > limiar \]
O principal benefício desse algoritmo é ser rápido e computacionalmente barato, contudo, além de não conseguir lidar bem com ruído, mudanças bruscas de iluminação e movimentos periódicos no fundo, esse algoritmo faz com que um objeto que fique parado por mais de um quadro se torne parte do plano de fundo, fazendo com que isso seja considerado durante o processamento das informações da imagem.

\subsection{Diferença Entre Quadro e o Fundo da Cena}
Esse método é bastante simples também, difere do anterior no fato do fundo a ser calculado ser estático entre quadros. É usual que se defina o fundo como o quadro no instante $ t = 0 $, ou ainda efetuar edições na imagem utilizada como fundo para se obter o melhor resultado.

Assim como o método da diferença entre quadros, a diferença entre o quadro e o fundo da cena é computacionalmente barato, porém sofre dos mesmos males de não
conseguir lidar com ruído, mudanças bruscas de iluminação e movimentos periódicos dos objetos contidos no plano de fundo, contudo, esse algoritmo não sofre com a absorção de um objeto do primeiro plano pelo plano de fundo.

\subsection{Modelo de mistura de gaussianas}

Um modelo de mistura de gaussianas, ou (\textit{GMM - Gaussian Mixture Model}) é um modelo estocástico que modela classes, sem que se considerem informações temporais. Esse modelo probabilístico se mostrou bastante eficiente para se definir o que faz parte do plano de fundo de uma imagem, sendo uma das técnicas que obtém melhor resultado em subtração de fundo. Diversas implementações de BGS usando GMM foram efetuadas, dentre elas destacam-se \citen{Stauffer1999}, \citen{KaewTraKulPong2002}, \citen{Zivkovic2004} e \citen{Bouwmans2008}.

A implementação usada nesse caso foi a proposta por \citen{Zivkovic2004} que, junto da implementação de \citen{KaewTraKulPong2002}, foi implementada na biblioteca OpenCV. O modelo proposto, parte do pressuposto que, na prática, em uma cena externa, a iluminação se altera lentamente (normalmente por condições de tempo) e em uma cena interna, de uma maneira rápida (ligar ou desligar uma lâmpada). Um novo objeto pode ser inserido em uma cena, ou removido dela. Em função disso, é necessário que exista um treinamento do modelo de plano de fundo para que haja uma adaptação às alterações do mesmo, adicionando novas amostras ou removendo antigas. Em função disso, o algoritmo propõe que, para cada alteração de uma amostra, o valor do plano de fundo seja novamente estimado, para que seja identificada a introdução de um novo objeto no primeiro plano ou a absorção pelo plano de fundo.

%\section{Trabalhos Relacionados}

\chapter{A Técnica Proposta}

\section{Motivação}

Hoje em dia é bastante comum encontrar em Shoppings Centers sensores de estacionamento individuais, que indicam se uma vaga está ocupada ou não. Essa indicação muitas vezes economiza diversos minutos em busca de uma vaga para estacionar seu carro, principalmente quando estamos próximos das festas de final de ano. A solução usada nesses casos normalmente compreendem sensores de presença, infravermelho ou ultrassom e exigem que cada vaga seja monitorada por um sensor específico. No entanto, ao partirmos para estacionamentos descobertos (ou \textit{outdoor}), verificamos que não há sistema semelhante que venha a indicar vagas disponíveis e, devido à exposição às intempéries, a solução adotada em ambientes internos não é aplicável.

Essa ausência de dispositivos indicativos de vagas de estacionamento é percebida nos estacionamentos descobertos da PUCRS, aonde algumas vagas são levemente deslocadas do restante do estacionamento e, muitas vezes, acabam sendo preteridas dada a dificuldade de se encontrar espaços vagos. Conforme pode ser visto na Figura~\ref{fig:esta}.

\begin{figure}
	\centering
	\caption{Estacionamento E03 da PUCRS, visto do Prédio 99A}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/esta}
	\label{fig:esta}
	\legend{Fonte: O Autor}
\end{figure}

A percepção desse problema, unida ao sucesso obtido com os sensores de estacionamentos internos motivou esse trabalho, que busca utilizar de visão computacional para identificar as vagas disponíveis no estacionamento em questão, mantendo uma premissa básica: ser de baixo custo. Por isso, durante a avaliação do equipamento utilizado, se chegou ao uso do Raspberry Pi, que é um computador de pequenas dimensões e custo (cerca de 200 reais, no Brasil). 

\section{O Equipamento}

O Raspberry Pi é um mini-computador desenvolvido no Reino Unido pela Raspberry Pi Foundation, com o propósito de promover o ensino de ciência da computação em escolas. Lançado em 2012, tornou-se um sucesso devido ao seu baixo custo, no Brasil, o modelo mais atual é vendido hoje por pouco mais de 200 reais.

Inicialmente, esse projeto foi concebido para ser executado no Raspberry Pi B+, que foi lançado em 2014, contudo, em fevereiro de 2015, a Raspberry Pi Foundation lançou um novo modelo, chamado Rasberry Pi B Geração 2, com consideráveis melhorias, principalmente no que diz respeito a processamento e memória (ver tabela~\ref{tbl:rpi}). Considerando que os algoritmos aqui executados seriam consideravelmente custosos, o projeto foi repensado para fazer uso do novo equipamento.

\begin{table}
	\caption{Comparativo Raspberry Pi B+ e Raspberry Pi B Geração 2}
	\begin{center}
		\begin{tabular}{c|p{5cm}|p{5cm}}
			\textit{Recurso}  &   \textit{Raspberry Pi B+}  &   \textit{Raspberry Pi B Gen2} \\
			\hline
			\hline
			Valor & USD 25 & USD 35 \\
			\hline
			SOC & Broadcom BCM2835  & Broadcom BCM2836 \\
			\hline
			CPU & 700 MHz single-core ARM 1176JZF-S & 900 MHz quad-core ARM Cortex-A7 \\
			\hline
			Memória (SDRAM) & 512 MB & 1 GB \\
			\hline
			Potência máxima & 9W & 9W \\
			\hline
			Sistema Operacional & GNU/Linux & GNU/Linux; Windows 10 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: O Autor}
	\label{tbl:rpi}
\end{table}

\begin{figure}
	\centering
	\caption{Raspberry Pi B+ Geração 2}
	\includegraphics[width=0.6\textwidth]{../../../Dropbox/__TG1/_images/pi2}
	\label{fig:rpi}
	\legend{Fonte: Raspberry Pi Foundation}
\end{figure}


O dispositivo óptico utilizado no projeto foi uma Raspberry Pi NoIR camera (figura ~\ref{fig:picam}), que é uma câmera com sensor CMOS OmniVision OV5647, com resolução QSXGA (5-megapixel), com modificação "NoIR", ou seja, sem o filtro infra-vermelho, permitindo que a câmera capture ondas com comprimento próximos ao infra-vermelho (700 - 1000nm).

\begin{figure}
	\centering
	\caption{Raspberry Pi NoIR Camera}
	\includegraphics[width=0.6\textwidth]{../../../Dropbox/__TG1/_images/picamera}
	\label{fig:picam}
	\legend{Fonte: Raspberry Pi Foundation}
\end{figure}

No quesito software, o ambiente preparado executava Raspbian GNU/Linux e o projeto foi desenvolvido em C++ e Python, fazendo uso da biblioteca para visão computacional OpenCV.

\begin{figure}[h]
	\centering
	\caption{Protótipo utilizado}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/setup}
	\label{fig:setup}
	\legend{Fonte: O Autor}
\end{figure}

\section{Preparação do sistema}

O processo de validação do sistema segue uma ordem semelhante à disposta nesse trabalho: primeiramente foi obtida a matriz de calibração da câmera e os coeficientes de distorção, por meio do uso de um padrão de calibração, conforme disposto na seção ~\ref{calibpat}. O padrão utilizado foi um tabuleiro de xadrez com dimensões de $ 9 \times 6 $, como pode ser visto na figura~\ref{fig:calib2}, que foi uma das imagens utilizadas na calibração da câmera.

\begin{figure}[h]
	\centering
	\caption{Imagem do conjunto de calibração da câmera}
	\includegraphics[width=0.8\textwidth]{../../../Dropbox/__TG1/_images/calib2}
	\label{fig:calib2}
	\legend{Fonte: O Autor}
\end{figure}

Após isso, foi estimada a transformação necessária para obtermos a vista de pássaro, conforme a seção ~\ref{birds}, embora essa transformação não seja necessária, ela facilitaria bastante nesse trabalho, pois as vagas de estacionamento passariam a ser retangulares, facilitando bastante a manipulação de seus valores, o resultado pode ser visto na comparação entre as figuras ~\ref{fig:frame1} e ~\ref{fig:frame2}.

\begin{figure}
	\centering
	\caption{Transformações aplicadas na imagem}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step1}
		\caption{Imagem original}
		\label{fig:frame1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step2}
		\caption{Correção de distorções}
		\label{fig:frame2}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step3}
		\caption{\textit{Bird's eye view}}
		\label{fig:frame3}
	\end{subfigure}
	\label{fig:frame}
	\legend{Fonte: O Autor}
\end{figure}



Após termos a visão de pássaro, são delimitadas as regiões de interesse (ROIs) de cada vaga a ser avaliada. Por conveniência, essas regiões foram delimitadas como retângulos de 40 por 40 pixeis, sendo que as vagas são regiões retangulares de cerca de 75 por 55 pixeis. Foi optado por não utilizar todo o espaço da vaga para evitar que pedaços do chão façam parte do histograma, que aumentaria a incerteza no momento da decisão, esse valor foi escolhido observando-se a ocupação média dos veículos, de modo que todos os cenários observados fossem cobertos.

Uma vez que as regiões são demarcadas, suas coordenadas são armazenadas em disco para possibilitar sua recuperação.

\section{Proposta Original}

A primeira proposta de implementação era baseada na metodologia executada por \citen{Oliveira2008}, com a detecção dos veículos com o uso de classificadores Haar. Conforme discutido na seção~\ref{haar}, a eficiência desse tipo de classificadores depende diretamente da qualidade do conjunto de treinamento, e a criação de um novo é consideravelmente trabalhosa, em localizar as amostras e computacionalmente custosa ao gerar o conjunto de treinamento, geralmente se emprega o uso de um cluster no processo de computação de um conjunto de treinamento Haar.

Nessa etapa foi utilizado o \textit{haar-cascade} \textbf{cars3.xml} que foi treinado utilizando 526 imagens de carros, com 360x240 pixeis cada, sem escala, essas imagens foram extraídas de um conjunto de dados proposto por Brad Philip and Paul Updike, com imagens obtidas no sul da Califórnia \cite{Oliveira2008}.

Contudo, o conjunto de treinamento não se mostrou eficiente: de fato, as condições em que foram coletadas as imagens eram consideravelmente diferentes das que essa solução estará submetida. Portanto, dada a carga computacional necessária para se gerar conjunto de treinamento personalizado que estivesse de acordo com as imagens obtidas nesse projeto, essa proposta foi relegada.

Ainda nas propostas inseridas no Trabalho de Graduação 1 (Anexo~\ref{tg1}), foi considerado o uso de subtração de plano de fundo, pois, no tocante de definir a alteração da situação de uma das vagas em questão, poderia ser avaliado por meio de alterações no primeiro plano numa região delimitada e marcada como uma vaga. Nesse estudo foram utilizados métodos básicos, como diferença entre quadros, que, apesar de serem computacionalmente eficientes não retornam resultados relevantes, e Mistura de Gaussianas, de acordo com o proposto por \citen{Zivkovic2004}. No caso da Mistura de Gaussianas o resultado foi significativamente superior, contudo, onerando a carga computacional.

Outro ponto que pesava na utilização de subtração de fundo é que seriam detectadas somente alterações na situação de uma vaga, ou seja, em se pensando de um produto, seria necessário conhecer seu estado inicial. Devido a essa necessidade de se ter a situação inicial do estacionamento, concluiu-se que essa solução, por sí só não seria suficiente, contudo, também não deveria ser descartada.

Analisando, então, a proposta original contida no Trabalho de Graduação 1, vemos que ela, por si só, não é suficiente para a detecção de vagas de estacionamento, principalmente pela necessidade do conhecimento da situação inicial, em razão disso se fez necessária uma nova proposta de solução.

\section{Segunda Implementação}

Uma vez avaliada a insuficiência técnica da proposta inicial, foi preciso efetuar uma nova avaliação do problema, partindo de um novo pressuposto: identificar a presença de um automóvel em uma determinada região de pixeis, em um determinado instante de tempo, com a maior confiabilidade possível. O problema em si é simples, contudo, sua solução passa por identificar padrões no estacionamento que permitissem essa detecção.

Uma solução para o problema em questão foi proposta em \citen{True2007}, que utiliza um sisema de detecção de ocupação de vagas de estacionamento por meio de imagens estáticas. Nessa solução a identificação da ocupação das vagas de estacionamento é feita basicamente por meio da comparação de histograma dos canais de crominância (a* e b*) do espaço de cores CIELAB entre a região de interesse que é delimitada por uma vaga e um espaço sabidamente desocupado. Nessa proposta, True utilizou mecanismos de aprendizado de máquina para aumentar a confiabilidade dos resultados, além da detecção de \textit{features} nessas regiões.

Partiu-se então no processo de \textbf{detecção de vagas por análise de um quadro}, esta etapa consiste de se identificar a ocupação de vagas pré-delimitadas por meio de uma análise de um determinado quadro ($ frame(n) $). Nesse momento, busca se identificar de maneira consistente se uma vaga está ocupada ou disponível.

O processamento a ser executado consiste de sete etapas, cujo fluxograma de execução pode ser visto na figura~\ref{fig:fluxo}:
\begin{enumerate}
	\item Obtenção do quadro;
	\item Correção em função da distorção da câmera;
	\item Aplicação de homografia para vista de pássaro;
	\item Segmentação de vagas e regiões de comparação (piso);
	\item Conversão do espaço de cores: BGR para LAB;
	\item Obtenção dos histogramas das vagas e regiões de comparação;
	\item Comparação de histogramas.
\end{enumerate}

\begin{figure}
	\centering
	\caption{Fluxograma inicial}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/diag}
	\label{fig:fluxo}
	\legend{Fonte: O Autor}
\end{figure}

Uma vez tendo sido definido o processo de execução, se passou ao desenvolvimento do sistema em questão, primeiramente foi montada uma estrutura de classes para permitir que fosse feita a implementação da parte operacional. Nesse ponto foi necessário identificar quais seriam os pontos variáveis do sistema, de moto que ele fosse configurável em diversos testes. O esqueleto do sistema utilizado para validação está descrito no diagrama de classe UML da figura~\ref{fig:class}.

\begin{figure}
	\centering
	\caption{Diagrama de classe}
	\includegraphics[width=0.6\textwidth]{../../../Dropbox/__TG1/_images/class}
	\label{fig:class}
	\legend{Fonte: O Autor}
\end{figure}

A partir do sistema instrumentado, iniciou-se o desenvolvimento do mecanismo de identificação da ocupação das vagas, por meio dos métodos aqui discutidos, essa implementação consistiu de implementar o método \textit{Lot::updateStatus()}, que seria o método responsável por atualizar a situação de cada vaga em função de um novo quadro. Nesse trabalho, esse processo foi executado de duas formas: Comparação de Histogramas e Contagem de \textit{Features}.

\subsection{Comparação de Histogramas}

Conforme disposto na seção ~\ref{ch:hist}, comparação de histogramas é um método bastante eficiente de se detectar similaridade entre imagens. Portanto, tendo um espaço aonde sabidamente possuímos o chão, podemos verificar sua similaridade com cada uma das vagas delimitadas. Conforme proposto por \citen{True2007}, a comparação foi feita utilizando os canais de crominância (a*b*) do modelo de cores CIELAB. Cada 
histograma foi gerado com $ 16 \times 16 $ classes (\textit{bins}), ou seja, com 256 classes.

A partir da figura~\ref{fig:frame3}, foram delimitadas áreas que correspondem a vagas e algumas áreas de piso, conforme pode ser verificado na figura~\ref{fig:framemark}. Nessa imagem foram selecionados três segmentos para carros, em ciano, e três para asfalto, em vermelho, mapeados, posteriormente para as figuras ~\ref{fig:histcar} e~\ref{fig:histasf}, respectivamente.

Para cada segmento selecionado, foi efetuada a conversão para CIE L*a*b*, para facilitar o descarte da luminância e, a partir dos canais a* e b* foi gerado o histograma que está plotado no gráfico de dispersão, representado pelas figuras ~\ref{fig:histcar}c e~\ref{fig:histasf}c. Aonde a componente horizontal corresponde ao canal a*, indo de verde para vermelho (esquerda para a direita) e vertical o canal b*, indo de amarelo para azul (baixo para cima).

\begin{figure}
	\centering
	\caption{Figura~\ref{fig:frame3} com os patches selecionados}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/frame_step3m}
	\label{fig:framemark}
	\legend{Fonte: O Autor}
\end{figure}

\begin{figure}[h]
	\centering
	\caption{Sequência para cálculo de histograma: Carros}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_0}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_0}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_0}
		\label{}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_1}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_1}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_1}
		\label{}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_2}
		\label{}
		\caption{Imagem original em RGB \linebreak}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_2}
		\label{}
		\caption{Imagem convertida para LAB}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_2}
		\label{}
		\caption{Gráfico de dispersão do histograma dos canais a* e b*}
	\end{subfigure}
	\label{fig:histcar}
	\legend{Fonte: O Autor}
\end{figure}

\begin{figure}[h]
	\centering
	\caption{Sequência para cálculo de histograma: Piso}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_3}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_3}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_3}
		\label{}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_4}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_4}
		\label{}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_4}
		\label{}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step4_5}
		\label{}
		\caption{Imagem original em RGB \linebreak}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step5_5}
		\label{}
		\caption{Imagem convertida para LAB}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/frame_step6_5}
		\label{}
		\caption{Gráfico de dispersão do histograma dos canais a* e b*}
	\end{subfigure}
	\label{fig:histasf}
	\legend{Fonte: O Autor}
\end{figure}

Tendo disponíveis os histogramas de regiões conhecidas de asfalto e de outras que são delimitadas por vagas de estacionamento, é possível calcular a distância entre os dois histogramas e, conforme a diferença entre eles, determinar se a vaga está ocupada ou não em determinado instante. Considerando que o número de classes do histograma já foi previamente definido, fica necessário apenas determinar qual o método de cálculo da distância entre os dois histogramas que será usado, bem como o limiar que irá definir se uma vaga está ocupada ou não.

O método de comparação de histogramas é um dos pontos de maior liberdade no escopo deste projeto, todos os métodos podem ser utilizados sem problema, contudo, alguns possuem características que facilitam o mecanismo de tomada de decisão. De acordo com o discutido na seção~\ref{ch:hist}, dos três métodos, dois deles possuem o valor mais alto para o caso de imagens idênticas: \textbf{correlação} e \textbf{intersecção}.

Avaliando o comportamento dos métodos de comparação de histogramas, foi verificado que a \textbf{correlação} é o mais adequado, devido ao fato de uma combinação perfeita, isto é, dois histogramas iguais, sempre retornar o mesmo valor, independentemente do histograma em questão, o que não aconteceria com a intersecção.

\chapter{Resultados}

A metodologia de avaliação do algoritmo proposto foi feita utilizando-se o sistema descrito na figura~\ref{fig:class}, a partir de vagas de $ 40px \times 40px $. Cada uma dessas vagas foi comparada a uma região conhecida de piso, também de $ 40px \times 40px $. A seleção e o cálculo dos histogramas era feito a cada quadro, fazendo com que as regiões estivesse sob condições de iluminação semelhantes.

A partir do décimo quadro, era feita a demarcação das vagas ocupadas e sua corretude era determinada visualmente, em seguida eram passados cem quadros e uma nova análise feita. Vagas oclusas foram excluídas dessa análise por não estarem no escopo deste trabalho.

Em todos os testes foi utilizada a comparação de histogramas por correlação, com confiança de 80\%, portanto:
\begin{equation}
Status_{vaga}=\begin{cases}
livre, & \text{se $ D(vaga,patch) > 0,8 $}.\\
ocupado, & \text{senão}.
\end{cases}
\end{equation}

Os parâmetros variados foram, principalmente, a iluminação e a ocupação do estacionamento, como a confiança é alta, vemos que o sistema tem uma tendência maior a gerar falsos-negativos do que falsos-positivos.

Para cada cenário será exibida uma imagem ilustrativa, gerada a partir de um quadro do respectivo cenário, cujos dados foram utilizados na aferição dos resultados. Nesta imagem, um retângulo verde indica uma vaga detectada como disponível, um vermelho uma vaga detectada como ocupada e o retângulo branco indica a região usada para comparação.

\section{Primeiro Cenário: dia, alta ocupação}

Esse cenário foi definido num dia útil, por durante a tarde, tempo limpo. Nesse dia a região de interesse estava toda ocupada. Um quadro de exemplo está sendo mostrado na figura~\ref{fig:cen1} e os resultados estão dispostos na tabela~\ref{tbl:cen1}

\begin{figure}
	\centering
	\caption{Visualização do Primeiro Cenário}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/cen1}
	\label{fig:cen1}
	\legend{Fonte: O Autor}
\end{figure}

\begin{table}
	\caption{Resultados Primeiro Cenário}
	\begin{center}
		\begin{tabular}{c|c|c|c|c}
			\textit{Quadro}  &   \textit{Total de Vagas}  &   \textit{Acertos} & \textit{Erros} & \textit{\% acertos} \\
			\hline
			\hline
			10 & 49 & 40 & 9 & 81,63 \\
			\hline
			110 & 49 & 42 & 7 &  85,71 \\
			\hline
			210 & 49 & 4 & 81 & 83,67 \\
			\hline
			310 & 49 & 40 & 9 & 81,63 \\
			\hline
			410 & 49 & 42 & 7& 85,71 \\
			\hline
			510 & 49 & 43 & 6 & 87,76 \\
			\hline
			610 & 49 & 43 & 6 & 87,76 \\
			\hline
			710 & 49 & 43 & 6 & 87,76 \\
			\hline
			810 & 49 & 40 & 9 & 81,63 \\
			\hline
			Média & 49 & 41,56 & 7,44 & 84,81 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: O Autor}
	\label{tbl:cen1}
\end{table}


\section{Segundo Cenário: dia, baixa ocupação}

Esta aferição foi executada em um dia nublado, porém claro, no meio da tarde. Foi um final de semana, o estacionamento estava com baixa ocupação, essa situação está ilustrada na figura~\ref{fig:cen2} e os dados estão planificados na tabela~\ref{tbl:cen2}.

\begin{figure}
	\centering
	\caption{Visualização do Segundo Cenário}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/cen2}
	\label{fig:cen2}
	\legend{Fonte: O Autor}
\end{figure}

\begin{table}
	\caption{Resultados Segundo Cenário}
	\begin{center}
		\begin{tabular}{c|c|c|c|c}
			\textit{Quadro}  &   \textit{Total de Vagas}  &   \textit{Acertos} & \textit{Erros} & \textit{\% acertos} \\
			\hline
			\hline
			10 & 49 & 34 & 15 & 69,39 \\
			\hline
			110 & 49 & 35 & 14 &  71,43 \\
			\hline
			210 & 49 & 36 & 13 & 73,47 \\
			\hline
			310 & 49 & 40 & 9 & 81,63 \\
			\hline
			410 & 49 & 34 & 15 & 69,39 \\
			\hline
			510 & 49 & 34 & 15 & 69,39 \\
			\hline
			610 & 49 & 37 & 12 & 75,51 \\
			\hline
			710 & 49 & 31 & 18 & 63,23\\
			\hline
			810 & 49 & 35 & 14 & 71,43 \\
			\hline
			Média & 49 & 35,11 & 13,89 & 71,66 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: O Autor}
	\label{tbl:cen2}
\end{table}



\section{Terceiro Cenário: noite, baixa ocupação}

Essas imagens foram capturadas a noite, o estacionamento estava praticamente vazio, conforme mostrado na figura~\ref{fig:cen3}. Como podemos perceber, nessa imagem inclusive é difícil determinar visualmente quais são as vagas disponíveis. Além disso, dada a baixa ocupação, não é um bom cenário para validar a taxa de acerto do algoritmo.

\begin{figure}
	\centering
	\caption{Visualização do Terceiro Cenário}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/cen3}
	\label{fig:cen3}
	\legend{Fonte: O Autor}
\end{figure}


\chapter{Conclusões}

Analisando os resultados obtidos em cada cenário, fica claro que a solução por si só não é suficiente para efetuar a identificação com confiança da ocupação de vagas de estacionamento. No melhor caso, tivemos pouco mais de 82\% de acerto.

Outro ponto curioso é que a câmera do Raspberry Pi possui uma baixa capacidade de cores. Embora isso fosse esperado, pois esta câmera não possui filtro infravermelho, não era esperado que isso reduzisse tão drasticamente o espalhamento espectral de um objeto colorido, como pode ser visto na figura~\ref{fig:histcar}.

Foram efetuados alguns testes para tentar aumentar a taxa de acerto, como redução do valor de confiança para definir a ocupação de uma vaga e, ainda, foi adicionada inércia na alteração do estado de uma vaga, isso é, para o estado mudar, é necessário que ele permaneça estável por um número determinado de quadros. Contudo, ambas estratégias se mostraram pouco suficientes na melhoria da confiabilidade dos resultados.

Outra possibilidade é adicionar algoritmos de aprendizado de máquina para definir a ocupação ou não, ao invés de uma simples comparação de histogramas, como, por exemplo, \textit{k-Nearest-Neighbor} ou \textit{support vector machines}.

Logo, concluímos que o algoritmo proposto não oferece a confiabilidade necessária a um sistema real, contudo, pode ser utilizado como parte de uma solução maior para atingir tal objetivo.

%\chapter{Trabalhos Futuros}

\nocite{Sobral2014}
\nocite{6264659}
\nocite{KaewTraKulPong2002}
\nocite{BinZabawi2013}
\nocite{Lin2006}
\nocite{Brainard2003}
\bibliographystyle{abntex2-alf}
\bibliography{export}


\annex
\chapter{Autorização para Obtenção de Imagens}
\label{autoriz}
\begin{figure}[h]
	\caption{Reprodução da autorização de captura de imagens da PUCRS}
	\centering
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/autor}
	\label{fig:autor}
\end{figure}


\chapter{Trabalho de Graduação 1}
\label{tg1}

\end{document}

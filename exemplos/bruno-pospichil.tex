%
% exemplo genérico de uso da classe iiufrgs.cls
% $Id: iiufrgs.tex,v 1.1.1.1 2005/01/18 23:54:42 avila Exp $
%
% This is an example file and is hereby explicitly put in the
% public domain.
%
\documentclass[ecp,tc]{iiufrgs}
% Para usar o modelo, deve-se informar o programa e o tipo de documento.
% Programas :
%   * cic       -- Graduação em Ciência da Computação
%   * ecp       -- Graduação em Ciência da Computação
%   * ppgc      -- Programa de Pós Graduação em Computação
%   * pgmigro   -- Programa de Pós Graduação em Microeletrônica
%   
% Tipos de Documento:
%   * tc                -- Trabalhos de Conclusão (apenas cic e ecp)
%   * diss ou mestrado  -- Dissertações de Mestrado (ppgc e pgmicro)
%   * tese ou doutorado -- Teses de Doutorado (ppgc e pgmicro)
%   * ti                -- Trabalho Individual (ppgc e pgmicro)
% 
% Outras Opções:
%   * english    -- para textos em inglês
%   * openright  -- Força início de capítulos em páginas ímpares (padrão da
%                   biblioteca)
%   * oneside    -- Desliga frente-e-verso
%   * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def


% Use unicode
\usepackage[utf8]{inputenc}   % pacote para acentuação

% Necessário para incluir figuras
\usepackage{graphicx}           % pacote para importar figuras


\usepackage{times}              % pacote para usar fonte Adobe Times

\usepackage{amsmath}
% \usepackage{palatino}
% \usepackage{mathptmx}          % p/ usar fonte Adobe Times nas fórmulas

\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações abnt

\usepackage{graphicx}
\usepackage{caption}% itens individuais da nominata podem ser redefinidos com os comandos
% abaixo:
% \renewcommand{\nominataReit}{Prof\textsuperscript{a}.~Wrana Maria Panizzi}
% \renewcommand{\nominataReitname}{Reitora}
% \renewcommand{\nominataPRE}{Prof.~Jos{\'e} Carlos Ferraz Hennemann}
% \renewcommand{\nominataPREname}{Pr{\'o}-Reitor de Ensino}
% \renewcommand{\nominataPRAPG}{Prof\textsuperscript{a}.~Joc{\'e}lia Grazia}
% \renewcommand{\nominataPRAPGname}{Pr{\'o}-Reitora Adjunta de P{\'o}s-Gradua{\c{c}}{\~a}o}
% \renewcommand{\nominataDir}{Prof.~Philippe Olivier Alexandre Navaux}
% \renewcommand{\nominataDirname}{Diretor do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataCoord}{Prof.~Carlos Alberto Heuser}
% \renewcommand{\nominataCoordname}{Coordenador do PPGC}
% \renewcommand{\nominataBibchefe}{Beatriz Regina Bastos Haro}
% \renewcommand{\nominataBibchefename}{finalBibliotec{\'a}ria-chefe do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataChefeINA}{Prof.~Jos{\'e} Valdeni de Lima}
% \renewcommand{\nominataChefeINAname}{Chefe do \deptINA}
% \renewcommand{\nominataChefeINT}{Prof.~Leila Ribeiro}
% \renewcommand{\nominataChefeINTname}{Chefe do \deptINT}

% A seguir são apresentados comandos específicos para alguns
% tipos de documentos.

% Relatório de Pesquisa [rp]:
% \rp{123}             % numero do rp
% \financ{CNPq, CAPES} % orgaos financiadores

% Trabalho Individual [ti]:
% \ti{123}     % numero do TI
% \ti[II]{456} % no caso de ser o segundo TI

% Monografias de Especialização [espec]:
% \espec{Redes e Sistemas Distribuídos}      % nome do curso
% \coord[Profa.~Dra.]{Weber}{Taisy da Silva} % coordenador do curso
% \dept{INA}     
\usepackage{subcaption}
%
% Informações gerais
%
\title{Sistema de Baixo Custo para Detecção de Vagas de Estacionamento por Visão Computacional}

\author{Pospichil}{Bruno Meybom}
% alguns documentos podem ter varios autores:
%\author{Flaumann}{Frida Gutenberg}
%\author{Flaumann}{Klaus Gutenberg}

% orientador e co-orientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Jung}{Claudio Rosito}
%\coadvisor[Prof.~Dr.]{Knuth}{Donald Ervin}

% a data deve ser a da defesa; se nao especificada, são gerados
% mes e ano correntes
\date{julho}{2015}

% o local de realização do trabalho pode ser especificado (ex. para TCs)
% com o comando \location:
\location{Porto Alegre}{RS}

%
% palavras-chave
% iniciar todas com letras minúsculas, exceto no caso de abreviaturas
%
\keyword{Visão computacional}
\keyword{Estacionamento}
\keyword{Processamento de imagem}
\keyword{Veículos}

%
% inicio do documen\nocite{*}to
%
\begin{document}

% folha de rosto
% às vezes é necessário redefinir algum comando logo antes de produzir
% a folha de rosto:
% \renewcommand{\coordname}{Coordenadora do Curso}
\maketitle

% dedicatoria
\clearpage
\begin{flushright}
\mbox{}\vfill
{\sffamily\itshape
``Não seremos limitados pela informação que temos.\\
Seremos limitados por nossa habilidade de processar esta informação.''\\}
--- \textsc{Peter Drucker}
\end{flushright}

% agradecimentos
\chapter*{Agradecimentos}
Agradeço ao \LaTeX\ por não ter vírus de macro\ldots



% resumo na língua do documento
\begin{abstract}
Um dos maiores problemas das grandes cidades hoje em dia é, sem dúvidas, o gerenciamento de uma crescente frota de veículos, que dobrou nos últimos dez anos. Com esse crescimento, se torna cada vez mais complicado encontrar um espaço para deixar seu carro, inclusive em espaços reservados para esse fim - os estacionamentos. Em razão disso, cada vez mais encontramos soluções que buscam reduzir ao máximo o tempo de busca por uma vaga, é comum encontrar em shoppings soluções que indicam a ocupação individual de cada vaga, com luzes características, em estacionamentos internos. No entanto, quando o espaço está sujeito às intempéries, não encontramos sistemas semelhantes.

Esse trabalho se propõe a verificar possíveis soluções para esse caso, avaliando publicações recentes da área de visão computacional e processamento de imagem, sempre buscando utilizar-se de equipamentos de baixo custo.
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{Low-Cost System to Detect Parking Spaces Using Computer Vision}{Parking spot, Computer vision, OpenCV, Parking space, Parking, Color histogram}
This document is an example on how to prepare documents at II/UFRGS
using the \LaTeX\ classes provided by the UTUG\@. At the same time, it
may serve as a guide for general-purpose commands. \emph{The text in
the abstract should not contain more than 500~words.}
\end{englishabstract}

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{SPMD}
        \item[DLT] Direct Linear Transform
        \item[CV] Computer Vision
        \item[RGB] Red, Green and Blue
        \item[BGR] Blue, Green and Red
        \item[BGRA] Blue, Green, Red and Alpha
        \item[CIE] Commission Internationale de L'éclairage
        \item[PMS] Pantone Matching System
        \item[NCS] Natural Color System
        \item[USD] United States Dollar
        \item[MB] Megabytes ($ 10\times10^6 $ bytes)
        \item[GB] Gigabytes ($ 10\times10^9 $ bytes)
        \item[BGS] Background Subtraction
        \item[RCA] Radio Corporation of America
        \item[ROI] Region of Interest
        \item[GMM] Gaussian Mixture Model
        \item[PUCRS] Pontifícia Universidade Católica do Rio Grande do Sul
        \item[HSV] Hue, saturation and value
        \item[HSL] Hue, saturation and lightness
        \item[RPI] Raspberry Pi
        \item[CMOS] Complementary Metal-Oxide Semiconductor
        \item[QXGA] Quantum Extended Graphics Array
    \end{listofabbrv}

% idem para a lista de símbolos
%\begin{listofsymbols}{$\alpha\beta\pi\omega$}
%       \item[$\sum{\frac{a}{b}}$] Somatório do produtório
%       \item[$\alpha\beta\pi\omega$] Fator de inconstância do resultado
%\end{listofsymbols}

% lista de figuras
\listoffigures

% lista de tabelas
\listoftables

% sumario
\tableofcontents

% aqui comeca o texto propriamente dito

% introducao
\chapter{Introdução}
No início dos tempos, Donald E. Knuth criou o \TeX. Algum tempo depois, Leslie Lamport criou o \LaTeX. Graças a eles, não somos obrigados a usar o Word nem o LibreOffice.

\chapter{Fundamentação Teórica e Trabalhos Relacionados}

\section{Modelos de Câmeras}

\subsection{Câmera Esteonopeica (\textit{pinhole})}
O  modelo  de  câmera  estenopeica  é  o  mais  simples  existente,  proposto  por  Filippo Brunelleschi, no início do século XV, é matematicamente conveniente e, apesar de sua simplicidade, provê uma aproximação aceitável do processo de captura de imagem.

Imagine uma caixa, aonde existe um pequeno furo (do grego \textit{stenopo}, por isso estenopeica) no centro de uma das faces e sua face oposta sendo substituída por uma superfície translúcida. Se você colocar essa caixa em uma sala com pouca iluminação, com alguma fonte luminosa em frente à superfície com o furo, a imagem dessa fonte luminosa será exibida invertida na face translúcida, conforme pode ser visto na Figura~\ref{fig:image1}.

\begin{figure}
\centering
\caption{Modelo de câmera estenopeica}
\includegraphics{../../../Dropbox/__TG1/_images/image1}
\label{fig:image1}
\legend{Fonte: \citeauthor{Forsyth2011}}
\end{figure}

Esse modelo define um mapeamento geométrico do mundo 3D para o plano de imagem 2D, conhecido como projeção perspectiva, que cria uma imagem invertida e, as vezes é conveniente se pensar numa imagem virtual, criada em um plano entre a face contendo o furo e a imagem original, à mesma distância entre essa mesma face e o plano da imagem. Conforme o contexto, pode ser útil pensar na imagem virtual ou na real.

A projeção em perspectiva gera alguns efeitos na imagem projetada: o tamanho aparente dos objetos depende da sua distância, os mais distantes parecem menores do que os mais próximos.

\begin{figure}
	\centering
	\caption{Distorção de tamanho}
	\includegraphics{../../../Dropbox/__TG1/_images/image2}
	\label{fig:image2}
	\legend{Fonte: \citeauthor{Forsyth2011}}
\end{figure}

Outro efeito gerado na projeção de imagem é que duas retas paralelas no mundo, isso é, cuja intersecção ocorreria em uma distância infinita, se intersectem no horizonte da imagem. Tais coordenadas da intersecção na imagem são chamados pontos de fuga (\textit{vanishing points}). O conjunto dos pontos de fuga correspondentes a retas paralelas em um plano formam uma reta, conhecida como reta de fuga (\textit{vanishing line}).

\begin{figure}
	\centering
	\caption{Retas de fuga}
	\includegraphics{../../../Dropbox/__TG1/_images/image3}
	\label{fig:image3}
	\legend{Fonte: \citeauthor{Hartley2003}}
\end{figure}

\subsection{Parâmetros de Câmeras}
Definimos como parâmetros de câmera as informações que são responsáveis pelo mapeamento de uma cena do mundo 3D para o plano da imagem 2D e entendemos por calibração de câmera a obtenção desses parâmetros. Os parâmetros são divididos em dois grupos: os \textbf{intrínsecos}, que modelam as características e configurações das lentes, do sensor e a geometria e montagem da câmera, e os \textbf{extrínsecos}, que modelam a pose (posicionamento e orientação da câmera no espaço).

\subsubsection{Parâmetros Intrínsecos}

\begin{figure}
	\centering
	\caption{Modelo de câmera estenopeica}
	\includegraphics{../../../Dropbox/__TG1/_images/image4}
	\label{fig:image4}
	\legend{Fonte: \citeauthor{Hartley2003}}
\end{figure}

Consideremos o modelo representado na Figura~\ref{fig:image4}, aonde \texttt{C} representa o centro de projeção, também conhecido como centro da câmera ou centro óptico. Vamos definir um sistema de coordenadas onde \texttt{C} seja a origem, e consideraremos o plano \texttt{Z = f}, que é conhecido como plano da imagem ou plano focal e está posicionado em frente ao centro de projeção e \texttt{f} é a distância focal. A linha que passa pelo centro da câmera e é perpendicular ao plano da imagem é chamada de eixo principal ou eixo óptico e o ponto onde esse eixo encontra o plano da imagem é conhecido como ponto principal (\texttt{p}).

A partir da Figura~\ref{fig:image4}, temos que um ponto no espaço de coordenadas $(X, Y, Z)^T$ é mapeado, por semelhança de triângulos, para um ponto no plano da imagem $ (f\dfrac{X}{Z}, f\dfrac{Y}{Z}) $.
Suprimindo a coordenada final da imagem, temos que $ (X, Y, Z)^T \mapsto (f\dfrac{X}{Z}, f\dfrac{Y}{Z})^T $ descreve  a  projeção  de  pontos  do  mundo  para  coordenadas  da  imagem.  Isso  é, mapeando de um espaço 3D para um 2D.

Se o mundo e os pontos da imagem são representados por vetores homogêneos, então, a projeção é facilmente expressada como uma relação linear de suas coordenadas homogêneas. A equação anterior pode ser escrita como uma multiplicação matricial:

\[\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \mapsto \begin{pmatrix} fX \\ fY \\ fZ \end{pmatrix} = \begin{bmatrix} f &  &  & 0 \\ & f & & 0 \\ & & 1 & 0 \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \]

Na matriz acima, assumimos que a origem das coordenadas no plano da imagem é o ponto principal. No entanto, isso pode não ser verdade, por isso, podemos considerar que $ (X, Y, Z)^T \mapsto (f\dfrac{X}{Z} + p_x, f\dfrac{Y}{Z} + p_y)^T $ aonde $ (p_x, p_y)^T $ são as coordenadas do ponto principal, conforme a figura~\ref{fig:image5}.

\begin{figure}
	\centering
	\caption{Sistema de coordenadas da imagem e da câmera}
	\includegraphics{../../../Dropbox/__TG1/_images/image5}
	\label{fig:image5}
	\legend{Fonte: \citeauthor{Hartley2003}}
\end{figure}

Assim, podemos escrever a equação em coordenadas homogêneas, chegando a
\[\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \mapsto \begin{pmatrix} fX+Zp_x \\ fY+Zp_y \\ fZ \end{pmatrix} = \begin{bmatrix} f &  & p_x & 0 \\ & f & p_y & 0 \\ & & 1 & 0 \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} \]
aonde definimos
\[K = \begin{bmatrix} f &  & p_x & 0 \\ & f & p_y & 0 \\ & & 1 & 0 \end{bmatrix}\]
ou, na forma concisa: $ x = K[I|0]x_{cam}$.

A matriz $ K $ é conhecida como \textit{matriz de calibração de câmera}. Na forma concisa, assumimos que a câmera estará posicionada na origem do sistema de coordenadas, com o eixo principal apontando para fora do eixo $ x $ e o ponto $ x_{cam} $ é definido nesse sistema de coordenadas.

\subsubsection{Parâmetros Extrínsecos}
\label{sec:extrinsic}
Os parâmetros extrínsecos relacionam o sistema de coordenadas da câmera com o do mundo. Os dois sistemas de coordenadas se relacionam por rotação e translação, conforme pode ser visto na figura~\ref{fig:image6}

\begin{figure}
	\centering
	\caption{Transformação entre o sistema de coordenadas da câmera e do mundo}
	\includegraphics{../../../Dropbox/__TG1/_images/image6}
	\label{fig:image6}
	\legend{Fonte: \citeauthor{Hartley2003}}
\end{figure}

Sendo $ \widetilde{X} $ um vetor não homogêneo representando um ponto no sistema de coordenadas do mundo e $ \widetilde{X}_{cam} $ representando o mesmo ponto no sistema de coordenadas da câmera, podemos escrever $ \widetilde{X}_{cam} = R(\widetilde{X} - \widetilde{C}) $ , aonde $ \widetilde{C} $ representa as coordenadas do centro da câmera no sistema de coordenadas do mundo e R é uma matriz $ 3 \times 3 $ representando a orientação do sistema de coordenadas da câmera, ou seja:
\[ X_{cam} =
\begin{bmatrix} R & -R\widetilde{C} \\ 
0 & 1 \end{bmatrix}
\begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} =
\begin{bmatrix} R & -R\widetilde{C} \\ 
0 & 1 \end{bmatrix}
\textbf{X}  \]

Que, por sua vez, unindo com a matriz de calibração da câmera, temos que \[x = KR[I|-\widetilde{C}]\textbf{X} \] onde \textbf{X} é dado no sistema de coordenadas do mundo.

Vemos que em uma câmera estenopeica $ P = KR[I|-\widetilde{C}] $ temos nove graus de liberdade: três para $ K(f, p_x, p_y) $, três para $ R $ e três para $ \widetilde{C} $. Os parâmetros de $K$ são conhecidos como \textit{parâmetros internos da câmera}, os
parâmetros $R$ e $\widetilde{C}$ relacionam a orientação da câmera e sua posição no sistema de coordenadas do mundo e são chamados \textit{parâmetros externos}.

Alem disso, pode ser útil deixar o centro da câmera como variável implícita. representando a transformação do sistema de coordenadas do mundo para a imagem como $ X_{cam} = R\widetilde{X} + t $. Nesse caso, a matriz da câmera se torna $ x = K[R|t] $, aonde $ t = -R\widetilde{C} $.

\subsubsection{Imperfeições de Lentes}

Os parâmetros intrínsecos e extrínsecos descritos anteriormente fornecem a descrição do processo de formação de imagem em uma câmera estenopeica, no entanto, ao lidarmos com câmeras de lente reais, encontramos algumas distorções que não foram previstas
até aqui. Considerando que estas são geradas pelo processo construtivo da lente, podemos considerá-las como um tipo específico de parâmetros intrínsecos.

De acordo com \citen{Hartley2003}, a modelagem exata das lentes é uma tarefa complexa, sendo que dessas imperfeições, a distorção radial (ou distorção barril) é a mais relevante em ser corrigida. Essa distorção provoca que uma reta no sistema de coordenadas do mundo seja projetada com uma curvatura no plano da imagem.

Estamos fazer.


\begin{figure}
	\centering
	\caption{Distorção radial}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/image7a}
		\caption{Mapa de pixeis sem distorção}
		\label{fig:image7a}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/image7b}
		\caption{Imagem com distorção radial}
		\label{fig:image7b}
	\end{subfigure}
	\label{fig:image7}
	\legend{Fonte: \citeauthor{Morvan2009}}
\end{figure}

De acordo com \citen{Morvan2009}, a relação entre a posição dos pixeis na imagem distorcida $ (x_d, y_d)^T $ e a imagem corrigida $ (x_u, y_u)^T $ é definida por
\[ \begin{bmatrix}x_u - p_x \\ y_u - p_y \end{bmatrix} = L(r_d)\begin{bmatrix}x_d - p_x \\ y_d - p_y \end{bmatrix}\]
onde $ (p_x, p_y)^T $ são as coordenadas do ponto principal e $ L(r_d) = 1+K_1{r_d}^2 $ em que $ k_1 $ é a quantidade de distorção radial presente na imagem e $ {r_d}^2 = (x_d - p_x)^2 + (y_d - p_x)^2 $.

A correção da distorção radial passa por definirmos os parâmetros $ k_1 $ e $ (p_x, p_y)^T $. Podemos estimar esses valores pelo cálculo da curvatura de uma linha na imagem 2D que corresponde a uma reta no espaço de coordenadas do mundo (3D). Normalmente se utiliza um padrão de calibração (muitas vezes um tabuleiro de xadrez), que consiste de uma sequência de retas conhecidas em coordenadas do mundo e, por meio dos valores obtidos em coordenadas da imagem, é possível estimar sua distorção.

\subsection{Calibração de Câmera}


O processo de calibração de câmera consiste em se estabelecer os parâmetros intrínsecos e extrínsecos que caracterizam a projeção da imagem, isso é, uma câmera é dita calibrada quando o mapeamento entre as coordenadas da imagem e as direções  relativas ao centro da câmera são conhecidas.

Isso nos conduz a definir uma série de valores (nesse caso, a matriz de calibração da câmera) que efetuam o mapeamento de um ponto no sistema de coordenadas do mundo real com um ponto na imagem, um dos algoritmos mais utilizados para fazer essa correspondência de pontos, é a Transformação Linear Direta ou DLT.

\subsubsection{Transformação Linear Direta}

No final da seção~\ref{sec:extrinsic} definimos a forma canônica da matriz de calibração de uma câmera estenopeica, se considerando apenas os parâmetros intrínsecos e extrínsecos (repare que a distorção ocasionada por imperfeições na lente utilizada não está sendo abordada nessa equação), que é $ P = KR[I|-\widetilde{C}] $ e que, por sua vez, admite como solução:

\[
P = \begin{bmatrix} f & & p_x \\ & f & p_y \\ & & 1 \end{bmatrix} [I|0] 
\begin{bmatrix} R & -R\widetilde{C} \\ 0 & 1 \end{bmatrix} = 
\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\ m_{21} & m_{22} & m_{23} & m_{24} \\ m_{31} & m_{32} & m_{33} & m_{34} \end{bmatrix}
\]

Em função disso as coordenadas da imagem e do mundo se relacionam por meio da matriz $ M $, ou seja,

\[
\begin{pmatrix} u_s \\ v_s \\ s \end{pmatrix} = \begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\ m_{21} & m_{22} & m_{23} & m_{24} \\ m_{31} & m_{32} & m_{33} & m_{34} \end{bmatrix} \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix}
\]

Onde $ \begin{pmatrix} X & Y & Z \end{pmatrix}^T $ representam coordenadas do mundo e $ \begin{pmatrix} u & v \end{pmatrix}^T $ coordenadas na imagem (pixeis).

\[
u = \dfrac{m_{11}X + m{12}Y + m_{13}Z + m_{14}}{m_{31}X + m{32}Y + m_{33}Z + m_{34}}
\]
\[
v = \dfrac{m_{21}X + m{22}Y + m_{23}Z + m_{24}}{m_{31}X + m{32}Y + m_{33}Z + m_{34}}
\]

Assim, para cada conjunto de valores $ (u, v, X, Y, Z) $ obtemos duas equações, considerando que existem doze incógnitas na matriz, precisaremos de seus conjuntos de valores, gerando doze equações, assumindo que elas são linearmente independentes),
chegando ao sistema de equações $ 12\times12 $.

\setcounter{MaxMatrixCols}{20}

\[
\begin{bmatrix}
X_1 & X_1 & X_1 & 1 & 0 & 0 & 0 & 0 & -u_1X_1 & -u_1Y_1 & -u_1Z_1 & -u_1 \\
0 & 0 & 0 & 0 & X_1 & X_1 & X_1 & 1 & -v_1X_1 & -v_1Y_1 & -v_1Z_1 & -v_1 \\
X_2 & X_2 & X_2 & 1 & 0 & 0 & 0 & 0 & -u_2X_2 & -u_2Y_2 & -u_2Z_2 & -u_2 \\
0 & 0 & 0 & 0 & X_2 & X_2 & X_2 & 1 & -v_2X_2 & -v_2Y_2 & -v_2Z_2 & -v_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
X_n & X_n & X_n & 1 & 0 & 0 & 0 & 0 & -u_nX_n & -u_nY_n & -u_nZ_n & -u_n \\
0 & 0 & 0 & 0 & X_n & X_n & X_n & 1 & -u_nX_n & -u_nY_n & -u_nZ_n & -v_n 
\end{bmatrix}
\begin{pmatrix}m_{11} \\ m_{12} \\ m_{13} \\ m_{14} \\ m_{21} \\ m_{22} \\ m_{23} \\ m_{24} \\ m_{31} \\ m_{32} \\ m_{33} \\ m_{34} \end{pmatrix} = 0
\]

Uma das características desse sistema é que múltiplos da matriz geram a mesma projeção, o que origina soluções diferentes da trivial no sistema acima. De fato, a solução trivial não é de interesse, pois gera uma matriz de projeção nula. Para obtermos uma solução diferente da trivial, é comum definirmos o valor de um dos parâmetros da matriz de projeção, comumente $ m_{34} = 1 $, fazendo com que o sistema se torne

\[
\begin{bmatrix}
X_1 & X_1 & X_1 & 1 & 0 & 0 & 0 & 0 & -u_1X_1 & -u_1Y_1 & -u_1Z_1 \\
0 & 0 & 0 & 0 & X_1 & X_1 & X_1 & 1 & -v_1X_1 & -v_1Y_1 & -v_1Z_1 \\
X_2 & X_2 & X_2 & 1 & 0 & 0 & 0 & 0 & -u_2X_2 & -u_2Y_2 & -u_2Z_2 \\
0 & 0 & 0 & 0 & X_2 & X_2 & X_2 & 1 & -v_2X_2 & -v_2Y_2 & -v_2Z_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
X_n & X_n & X_n & 1 & 0 & 0 & 0 & 0 & -u_nX_n & -u_nY_n & -u_nZ_n \\
0 & 0 & 0 & 0 & X_n & X_n & X_n & 1 & -u_nX_n & -u_nY_n & -u_nZ_n 
\end{bmatrix}
\begin{pmatrix}m_{11} \\ m_{12} \\ m_{13} \\ m_{14} \\ m_{21} \\ m_{22} \\ m_{23} \\ m_{24} \\ m_{31} \\ m_{32} \\ m_{33} \\ m_{34} \end{pmatrix} = \begin{pmatrix} u_1 \\ v_1 \\ u_2 \\ v_2 \\ \vdots \\ u_n \\ v_n \end{pmatrix}
\]

Assim, obtemos um sistema com 11 incógnitas do tipo $ \vec{A_m} = \vec{b} $ , que é sobre determinado, isso é, a matriz possui dimensão $ m \times n $ e $ m > n $ (nesse caso, $ m = 12 $ e $ n = 11$ ). Este tipo de sistema normalmente não possui uma solução exata, então desejamos buscar uma solução aproximada, que, por sua vez, pode ser encontrada utilizando-se o método dos mínimos quadrados. De fato, é comum usarmos mais do que 12 equações, pois quanto mais valores $ (u, v, X, Y, Z) $ forem obtidos, maior o número de equações utilizadas e mais diluído será o erro encontrado.

\subsubsection{Padrões de Calibração}

Como pudemos ver, o processo de calibração de câmera, principalmente ao se considerar a distorção das lentes, não é um processo dos mais simples de ser executado. Uma alternativa muito eficiente é a utilização de padrões de calibração.

Um padrão de calibração consiste na utilização de uma figura que possua bordas bem definidas, de modo que essas sejam facilmente detectáveis, usualmente utilizamos algo semelhante a um tabuleiro de xadrez, pois suas formas proporcionam essa detecção, como pode ser visto na figura~\ref{fig:calib}. Com esse objeto em mãos, podemos utilizar softwares para obter os parâmetros de calibração da câmera automaticamente, isso é, definir a matriz de calibração da câmera. Além disso, outro ponto importante é que a calibração radial é \textit{off-line}, isso é, pode ser efetuada com a câmera fora de sua posição final, pois não se alteram em caso de alteração da cena. Isso acaba sendo conveniente, pois poderemos instalar uma câmera já calibrada, ficando necessária somente a definição dos parâmetros relativos a cena (extrínsecos).

\begin{figure}
	\centering
	\caption{Exemplo de calibração de câmera}
	\includegraphics{../../../Dropbox/__TG1/_images/calib}
	\label{fig:calib}
	\legend{Fonte: OpenCV Tutorials}
\end{figure}

Uma das alternativas disponíveis é o \textit{Camera Calibration Toolbox for Matlab}, que é um \textit{framework} desenvolvido para ser executado no Matlab com diversos recursos de calibração de câmeras. Além disso, seu código já foi portado para a linguagem C, de modo que foi embutido no OpenCV, que é uma biblioteca para o desenvolvimento de aplicativos na área de visão computacional, originalmente desenvolvida pela Intel.

\subsection{Vista de pássaro}

Vista de pássaro ou \textit{bird's eye view} consiste na visualização de um objeto ou plano como se estivesse acima do mesmo. Usualmente se efetua matriz de homografia para efetuar a transformação para esse ponto de vista.

\begin{figure}
	\centering
	\caption{Vista de pássaro (\textit{bird's eye view})}
	\begin{subfigure}[b]{0.47\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/birds1}
		\caption{Visão de uma superfície plana}
		\label{fig:birds1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/birds2}
		\caption{Vista de pássaro}
		\label{fig:birds2}
	\end{subfigure}
	\label{fig:image7}
	\legend{Fonte: \citeauthor{Bradski2008}}
\end{figure}


Apesar de não ter utilizado essa terminologia, o processo de calibração de câmera consiste na obtenção de uma ou mais matrizes de homografia que efetuam a correção das distorções provocadas pelas lentes na cena. No caso da vista de pássaro, seria necessário, apenas, encontrar a transformação que mapeasse pontos coplanares para uma mesma perspectiva, o método utilizado é o mesmo para a calibração da câmera: DLT.


\section{Espaços de cores}

Cor é o resultado da percepção da luz que incide na retina em células foto-receptoras, conhecidas como cones. De acordo com \citen{Shapiro2001}, a percepção de cor depende, além das características físicas da luz, de um processamento complexo pelo olho e cérebro que integra as propriedades do estímulo com a sensibilidade do observador.

\citen{Feitosa-Santana2006}, relatou que existem registros de estudos para encontrar uma maneira de se organizar as cores desde a antiguidade. Dentre os primeiros registros de espaço de cores, pode-se destacar  o trabalho de Pitágoras (circa 570-500 a.C.), que criou um espaço de cores semicircular relacionando as notas da escala musical de tons e meio tons aos planetas que, por sua vez, eram representados por determinadas cores, como pode ser visto na figura~\ref{fig:pit}.

\begin{figure}
	\centering
	\caption{Modelo de espaço de cores de Pitágoras}
	\includegraphics{../../../Dropbox/__TG1/_images/pit}
	\label{fig:pit}
	\legend{Fonte: http://www.colorsystem.com/}
\end{figure}

A colorimetria se tornou tecnicamente útil quando, em 1931, o CIE padronizou as funções de correspondência de cores. determinadas pela sistematização das misturas de cores necessárias para um observador, em condições específicas de iluminação. A partir do experimento executado em 1931 foram definidos os espaços de cores CIE XYZ e RGB.

Hoje em dia existem uma série de espaços de cores que atendem a determinados interesses, alguns com finalidade comercial, outro de uso genérico. Os espaços definidos pelo CIE em 1931 são de uso genérico, aonde ainda podemos adicionar outros como o HSV, CIELAB, CIELuv. Dentre os espaços de cores de fins comerciais, os mais conhecidos são o Pantone (PMS) e o NMS, desenvolvido pelo Scandinavian Colour Institute.

No escopo desse trabalho, as imagens capturadas pelas câmeras utilizam o espaço de cores RGB (na verdade BGR, devido à especificação do OpenCV), contudo, se faz útil efetuar a conversão para outro espaço de cores aonde a iluminação (luminância) seja separada das informações de cores (crominância), cujos modelos mais utilizados são HSV, HSL, CIEXYZ, YCbCr, CIELAB, CIELUV, YUV. \citen{Lee2005} efetuou uma avaliação de similaridade de imagem por meio de intersecção de histogramas, uma técnica que será utilizada nesse trabalho e em seus resultados encontrou que o CIELAB obteve os melhores índices de acerto, por esse motivo, este será o espaço de cores utilizado.

\subsection{Modelo de Cores CIE 1931 RGB}

O modelo de cores aditivo CIE RGB, foi definido a partir dos experimentos executados em 1931 no CIE e foi constituído a partir da teoria de visão colorida tricromática, de Young-Helmholtz e no triângulo de cores de James Clerk Maxwell, representado na figura~\ref{fig:triangle}. Esse modelo acabou se tornando padrão \textit{de facto} devido a ser visualmente intuitivo, uma vez que a maior parte das cores pode ser formada a partir de uma combinação das três cores fundamentais, e esse modelo passou a ser utilizado em equipamentos comerciais em meados da década de 50, quando a RCA introduziu o modelo em seu televisor CT-100, o primeiro colorido da marca.

\begin{figure}
	\centering
	\caption{Triângulo de cores de Maxwell}
	\includegraphics{../../../Dropbox/__TG1/_images/triangle}
	\label{fig:triangle}
	\legend{Fonte: Appalachian State University - Psychology 3215}
\end{figure}

Apesar de ser amplamente difundido, muitas vezes quando estamos efetuando processamento das imagens, esse modelo de cor acaba não sendo a melhor escolha, principalmente pelo fato de que as variações de cor não serem uniformes. Além disso, por meio unicamente de adições não é possível se representar todas as variações de cores, sendo necessários valores negativos para R e G para se atingir toda a gama existente.

\subsection{Modelo de Cores CIE 1976 L*a*b*}

A percepção de pequenas variações entre cores é de suma importância em experimentos de percepção e especificação de cores. Um sistema de cores deve permitir que sejam representadas todas as cores com uma alta precisão. Em processamento de imagens é interessante que em um espaço de cores uma pequena perturbação em uma componente é aproximadamente igualmente perceptível em toda a gama do referido valor, esses modelos de cores são conhecidos como perceptualmente uniformes.

Em 1976 o CIE propôs o modelo L*a*b* (Lab ou CIELAB), que, por definição é perceptualmente uniforme, que é sua maior vantagem se comparado com o RGB de 1931, contudo, não é a única vantagem, com o CIELAB é possível representar toda a a gama de cores, além disso, esse sistema se aproxima bastante do da visão humana (\textit{human vision system}).

Diferentemente do RGB, aonde cada canal representa uma cor básica, no CIELAB temos dois tipos distintos de canais, o canal de luminância (iluminação) $ L* $ e os de crominância $a*$ e $b*$, aonde $a*$ representa as cores oponentes vermelha e verde e $b*$ amarelo e azul: valores positivos de $a$" indicam vermelho e negativos verde, já valores positivo de $b*$ indicam amarelo e negativos azul, como pode ser visto na figura~\ref{fig:cielab}.

\begin{figure}
	\centering
	\caption{Modelo do CIELAB}
	\includegraphics{../../../Dropbox/__TG1/_images/cielab}
	\label{fig:cielab}
	\legend{Fonte: JISC Digital Media}
\end{figure}

\section{Comparação de Histogramas}

Histograma, também conhecido como distribuição de intensidade, é uma representação gráfica distribuída em colunas de dados, a partir da visualização de um histograma é possível aferir a maneira de distribuição, simetria e dispersão dos dados. Em processamento de imagem, histogramas são usados para se observar as características de uma imagem. Outro ponto extremamente relevante que a comparação de histogramas entre duas imagens é uma das maneiras mais eficientes de se detectar a similaridade entre elas.

\citen{Liu2015}, faz o uso de comparação de histogramas para identificar similaridade em regiões de vídeo, primeiramente a região de interesse é dividida em segmentos (\textit{bins}), e o valor de cada amostra é computado:

\[
bin(k) = \sum_{i=0}^{M}\sum_{j=0}^{N}f_k(i,j),(0 \le k \le 255)
\]

\begin{equation}
f_k(i,j)=\begin{cases}
1, & \text{se $p(i,j) = k$}.\\
0, & \text{se $p(i,j) \neq k$}.
\end{cases}
\end{equation}

Aonde $ p(i,j) $ significa o valor do pixel $ i, j$ em determinado quadro, e $ k $ o valor desse pixel em determinado canal. Por simplicidade, iremos considerar o histograma em uma dimensão somente, contudo, é usual se utilizar histogramas em mais de uma dimensão.

Uma vez tendo o histograma criado, existem diversos métodos para se encontrar a distância (ou diferença) entre dois deles, \citen{Liu2015}, descreve três das mais utilizadas atualmente: correlação, chi-quadrado e intersecção, sendo que cada uma utiliza de uma equação característica:

\[
d_{correl}(H_1,H_2) =  \frac{\sum_I (H_1(I) - \bar{H_1}) (H_2(I) - \bar{H_2})}{\sqrt{\sum_I(H_1(I) - \bar{H_1})^2 \sum_I(H_2(I) - \bar{H_2})^2}}
\]
\[
\bar{H_k} =  \frac{1}{N} \sum _J H_k(J)
\]
\[
d_{chi-quad}(H_1,H_2) =  \sum _I  \frac{\left(H_1(I)-H_2(I)\right)^2}{H_1(I)}
\]
\[
d_{intersec}(H_1,H_2) =  \sum _I  \min (H_1(I), H_2(I))
\]

No caso da correlação, quanto maior o valor encontrado, maior a similaridade entre as duas imagens, elas forem iguais, o resultado será 1, assim como na intersecção, valores menores do que 1 indicam diferenças na imagem. Para chi-quadrado, teremos no caso de imagens idênticas 0 e valores maiores que esse indicam que existem diferenças entre elas. Na documentação do OpenCV encontramos um exemplo que demonstra os valores encontrados na comparação de histogramas, utilizando as figuras~\ref{fig:hist_0}, ~\ref{fig:hist_1} e ~\ref{fig:hist_2}, o resultado obtido está disposto na tabela~\ref{tbl:hist}.

\begin{figure}[h]
	\centering
	\caption{Figuras de exemplo para comparação de histogramas}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_0}
		\caption{Base}
		\label{fig:hist_0}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_1}
		\caption{Teste 1}
		\label{fig:hist_1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/hist_2}
		\caption{Teste 2}
		\label{fig:hist_2}
	\end{subfigure}
	\label{fig:hist}
	\legend{Fonte: OpenCV}
\end{figure}

\begin{table}[h]
	\caption{Resultados comparação de histogramas}
	\begin{center}
		\begin{tabular}{c|c|c|c}
			\textit{Método}  &   \textit{Base x Base}  &   \textit{Base x Teste 1} & \textit{Base x Teste 2}  \\
			\hline
			\hline
			Correlação & 1,000000 &	0,182073 & 0,120447 \\
			\hline
			Chi-quadrado & 0,000000 & 21,184536 & 49,273437 \\
			\hline
			Intersecção & 24,391548 & 3,889029 & 5,775088 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: OpenCV}
	\label{tbl:hist}
\end{table}


\section{Classificadores Haar}
Classificadores Haar (\textit{Haar Classifiers}), é um framework de detecção de objetos rápido e robusto, proposto por \citen{Viola2004}, esse classificador usa o conceito de \textit{feature}, que é uma parte de informação que é relevante para resolver determinado problema. \textit{Features} podem ser estruturas como bordas, pontos ou objetos. No caso em questão, temos o conceito de perfis Haar (\textit{Haar-like features}), que são padrões geométricos usados nas sub-janelas de detecção, consistem de imagens adjacentes e foram assim chamadas por sua semelhança com as ondulações Haar (\textit{Haar wavelets}).

\begin{figure}
	\centering
	\caption{Exemplos de perfis Haar}
	\includegraphics{../../../Dropbox/__TG1/_images/haar}
	\label{fig:haar}
	\legend{Fonte: \citeauthor{Viola2004}}
\end{figure}

O valor de um perfil Haar é dado pela soma dos pixeis da(s) área(s) sombreada(s), subtraído da soma dos pixeis da(s) área(s) branca(s). Considerando que
as formas são retangulares, seus valores podem ser computados rapidamente utilizando uma representação de imagem conhecida como \textbf{Imagem Integral}.
A imagem integral no pixel x, y contém a soma dos pixeis acima e à esquerda de x, y, inclusive:
\[ ii(x,y) = \sum_{x'\le x,y' \le y} i(x', y') \]

Calculado o valor de cada perfil Haar, um classificador fraco é definido por:
\begin{equation}
	h(x)=\begin{cases}
		1, & \text{se $pf(x) < p\theta$}.\\
		0, & \text{senão}.
	\end{cases}
\end{equation}

Aonde $ x $ é uma sub-janela, $ \theta $ é um limiar e $ p $ define a paridade, indicando a direção do sinal da inequação.

Além do uso de perfis Haar, classificadores Haar utilizam AdaBoost (\textit{adaptive boosting} ou estímulo adaptável), que é um algoritmo de aprendizado de máquina proposto por \citen{Freund1997} que tem como objetivo aumentar a performance, construindo um classificador forte utilizando um conjunto de treinamento e um algoritmo de aprendizado fraco.

A utilização de Classificadores Haar depende da existência de um conjunto de treinamento que tenha um bom desempenho na cena que está sendo capturada. Um uso muito comum desses algoritmos é no reconhecimento de face, uma vez que normalmente esse tipo de captura está sob mesma perspectiva, seu uso em imagens de diferentes perspectivas pode vir a introduzir uma baixa taxa de acerto.

\section{Subtração de Fundo}
Subtração de fundo (ou \textit{background subtraction}, ou \textit{foreground detection}) é um passo fundamental em muitas aplicações no campo de visão computacional e processamento digital de imagens. Consiste, em geral, na comparação de uma imagem com outra imagem que modela o plano de fundo \textit{background}.  A diferença observada consiste de objetos no primeiro plano do vídeo (\textit{foreground}). Fazem parte do primeiro plano todos os objetos que não estão fixos em uma cena. Portanto, podemos definir em linhas gerais a função de detecção como: 
\[ |quadro_i - fundo| > limiar \]

Ao analisar a função, verificamos que existe um limiar (\textit{threshold}) a ser definido  que irá determinar se o pixel em questão faz parte do plano de fundo ou do primeiro plano. Se o valor for mal escolhido teremos resultados insatisfatórios: se o limiar definido for muito alto, poderão ser ignoradas algumas alterações relevantes no quadro e, se for muito baixo, fará com que sejam considerados ruídos desprezíveis.

\begin{figure}
	\centering
	\caption{Definição do \textit{threshold} para \textit{background subtraction}}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs1}
		\caption{Imagem original}
		\label{fig:bgs1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs2}
		\caption{Diferença absoluta}
		\label{fig:bgs2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs3}
		\caption{Limiar muito alto}
		\label{fig:bgs3}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../../Dropbox/__TG1/_images/bgs4}
		\caption{Limiar muito baixo}
		\label{fig:bgs4}
	\end{subfigure}
	\label{fig:imagebgs}
	\legend{Fonte: \citeauthor{Piccardi2004}}
\end{figure}

Atualmente contamos com dezenas de métodos disponíveis para executar o processo de subtração de fundo, que podem ser divididos em diversos grupos em função dos algoritmos utilizados:
\begin{itemize}
\item básicos: baseados em média e variância ao longo do tempo;
\item lógica fuzzy;
\item gaussianas simples;
\item múltiplas gaussianas
\item entre outros...
\end{itemize}


\citen{Sobral2013} descreveu o desenvolvimento de um \textit{framework} baseado em OpenCV que proporciona a inclusão de subtração de fundo de maneira extremamente fácil, a \textit{BGSLibrary}, a partir dessa biblioteca, foram selecionados alguns algoritmos para que fosse avaliada, num primeiro momento, a quantidade de ruído encontrada nas imagens de \textit{foreground}. Os métodos selecionados são descritos a seguir.

\subsection{Diferença Entre Quadros}

É um dos métodos mais simples de subtração de fundo (talvez o mais simples), nesse algoritmo, temos como o fundo sendo o valor do quadro anterior, ou seja:
\[|quadro_i - quadro_{i-1} | > limiar \]
O principal benefício desse algoritmo é ser rápido e computacionalmente barato, contudo, além de não conseguir lidar bem com ruído, mudanças bruscas de iluminação e movimentos periódicos no fundo, esse algoritmo faz com que um objeto que fique parado por mais de um quadro se torne parte do plano de fundo, fazendo com que isso seja considerado durante o processamento das informações da imagem.

\subsection{Diferença Entre Quadro e o Fundo da Cena}
Esse método é bastante simples também, difere do anterior no fato do fundo a ser calculado ser estático entre quadros. É usual que se defina o fundo como o quadro no instante $ t = 0 $, ou ainda efetuar edições na imagem utilizada como fundo para se obter o melhor resultado.

Assim como o método da diferença entre quadros, a diferença entre o quadro e o fundo da cena é computacionalmente barato, porém sofre dos mesmos males de não
conseguir lidar com ruído, mudanças bruscas de iluminação e movimentos periódicos dos objetos contidos no plano de fundo, contudo, esse algoritmo não sofre com a absorção de um objeto do primeiro plano pelo plano de fundo.

\subsection{Modelo de mistura de gaussianas}

Um modelo de mistura de gaussianas, ou (\textit{GMM - Gaussian Mixture Model}) é um modelo estocástico que modela classes, sem que se considerem informações temporais. Esse modelo probabilístico se mostrou bastante eficiente para se definir o que faz parte do plano de fundo de uma imagem, sendo uma das técnicas que obtém melhor resultado em subtração de fundo. Diversas implementações de BGS usando GMM foram efetuadas, dentre elas destacam-se \citen{Stauffer1999}, \citen{KaewTraKulPong2002}, \citen{Zivkovic2004} e \citen{Bouwmans2008}.

A implementação usada nesse caso foi a proposta por \citen{Zivkovic2004} que, junto da implementação de \citen{KaewTraKulPong2002}, foi implementada na biblioteca OpenCV. O modelo proposto, parte do pressuposto que, na prática, em uma cena externa, a iluminação se altera lentamente (normalmente por condições de tempo) e em uma cena interna, de uma maneira rápida (ligar ou desligar uma lâmpada). Um novo objeto pode ser inserido em uma cena, ou removido dela. Em função disso, é necessário que exista um treinamento do modelo de plano de fundo para que haja uma adaptação às alterações do mesmo, adicionando novas amostras ou removendo antigas. Em função disso, o algoritmo propõe que, para cada alteração de uma amostra, o valor do plano de fundo seja novamente estimado, para que seja identificada a introdução de um novo objeto no primeiro plano ou a absorção de um objeto pelo plano de fundo.

\section{Trabalhos Relacionados}

\chapter{A Técnica Proposta}

\section{Motivação}

Hoje em dia é bastante comum encontrar em Shoppings Centers sensores de estacionamento individuais, que indicam se uma vaga está ocupada ou não. Essa indicação muitas vezes economiza diversos minutos em busca de uma vaga para estacionar seu carro, principalmente quando estamos próximos das festas de final de ano. A solução usada nesses casos normalmente compreendem sensores de presença, infravermelho ou ultrassom e exigem que cada vaga seja monitorada por um sensor específico. No entanto, ao partirmos para estacionamentos descobertos (ou \textit{outdoor}), verificamos que não há sistema semelhante que venha a indicar vagas disponíveis e, devido à exposição às intempéries, a solução adotada em ambientes internos não é aplicável.

Essa ausência de dispositivos indicativos de vagas de estacionamento é percebida nos estacionamentos descobertos da PUCRS, aonde algumas vagas são levemente deslocadas do restante do estacionamento e, muitas vezes, acabam sendo preteridas dada a dificuldade de se encontrar espaços vagos. Conforme pode ser visto na Figura~\ref{fig:esta}.

\begin{figure}
	\centering
	\caption{Estacionamento E03 da PUCRS, visto do Prédio 99A}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/esta}
	\label{fig:esta}
	\legend{Fonte: O Autor}
\end{figure}

A percepção desse problema, unida ao sucesso obtido com os sensores de estacionamentos internos motivou esse trabalho, que busca utilizar de visão computacional para identificar as vagas disponíveis no estacionamento em questão, mantendo uma premissa básica: ser de baixo custo. Por isso, durante a avaliação do equipamento utilizado, se chegou ao uso do Raspberry Pi, que é um computador de pequenas dimensões e custo (cerca de 200 reais, no Brasil). 

\section{O Equipamento}

O Raspberry Pi é um mini-computador desenvolvido no Reino Unido pela Raspberry Pi Foundation, com o propósito de promover o ensino de ciência da computação em escolas. Lançado em 2012, tornou-se um sucesso devido ao seu baixo custo, no Brasil, o modelo mais atual é vendido hoje por pouco mais de 200 reais.

Inicialmente, esse projeto foi concebido para ser executado no Raspberry Pi B+, que foi lançado em 2014, contudo, em fevereiro de 2015, a Raspberry Pi Foundation lançou um novo modelo, chamado Rasberry Pi B Geração 2, com consideráveis melhorias, principalmente no que diz respeito a processamento e memória (ver tabela~\ref{tbl:rpi}). Considerando que os algoritmos aqui executados seriam consideravelmente custosos, o projeto foi repensado para fazer uso do novo equipamento.

\begin{table}
	\caption{Comparativo Raspberry Pi B+ e Raspberry Pi B Geração 2}
	\begin{center}
		\begin{tabular}{c|p{5cm}|p{5cm}}
			\textit{Recurso}  &   \textit{Raspberry Pi B+}  &   \textit{Raspberry Pi B Gen2} \\
			\hline
			\hline
			Valor & USD 25 & USD 35 \\
			\hline
			SOC & Broadcom BCM2835  & Broadcom BCM2836 \\
			\hline
			CPU & 700 MHz single-core ARM 1176JZF-S & 900 MHz quad-core ARM Cortex-A7 \\
			\hline
			Memória (SDRAM) & 512 MB & 1 GB \\
			\hline
			Potência máxima & 9W & 9W \\
			\hline
			Sistema Operacional & GNU/Linux & GNU/Linux; Windows 10 \\
			\hline
		\end{tabular}
	\end{center}
	\legend{Fonte: O Autor}
	\label{tbl:rpi}
\end{table}

\begin{figure}
	\centering
	\caption{Raspberry Pi B+ Geração 2}
	\includegraphics[width=0.6\textwidth]{../../../Dropbox/__TG1/_images/pi2}
	\label{fig:rpi}
	\legend{Fonte: Raspberry Pi Foundation}
\end{figure}


O dispositivo óptico utilizado no projeto foi uma Raspberry Pi NoIR camera (figura ~\ref{fig:picam}), que é uma câmera com sensor CMOS OmniVision OV5647, com resolução QSXGA (5-megapixel), com modificação "NoIR", ou seja, sem o filtro infra-vermelho, permitindo que a câmera capture ondas com comprimento próximos ao infra-vermelho (700 - 1000nm).

\begin{figure}
	\centering
	\caption{Raspberry Pi NoIR Camera}
	\includegraphics[width=0.6\textwidth]{../../../Dropbox/__TG1/_images/picamera}
	\label{fig:picam}
	\legend{Fonte: Raspberry Pi Foundation}
\end{figure}

No quesito software, o ambiente preparado executava Raspbian GNU/Linux e o projeto foi desenvolvido em C++ e Python, fazendo uso da biblioteca para visão computacional OpenCV.

\begin{figure}
	\centering
	\caption{Protótipo utilizado}
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/setup}
	\label{fig:setup}
	\legend{Fonte: O Autor}
\end{figure}

\section{Preparação do sistema}



\section{Proposta Original}

Partindo da imagem de uma câmera calibrada, com todos os parâmetros conhecidos, cujas regiões de interesse (ROIs), no caso, as vagas, seriam manualmente definidas, seria feita a detecção da ocupação das mesmas utilizando-se  \textit{Haar-Cascades}, conforme proposto por \citen{Oliveira2008} e \textit{Background Subtraction}, que pode ser verificado no Trabalho de Graduação 1 (Anexo~\ref{tg1}).

\textit{Haar-Cascades} se mostrou ineficiente na solução desse problema uma vez que não foi possível encontrar um conjunto de treinamento suficiente para o cenário em questão e, inicialmente, gerar um novo conjunto de treinamento estava fora de cogitação pela carga computacional necessária. Apesar dessa técnica ser bem eficiente em identificar por padrões treinados em imagens, infelizmente, não dispondo de um conjunto de treinamento adequado, se mostra ineficiente.

O uso de \textit{background subtraction}, que também foi um dos algoritmos que foram considerados originalmente, foi amplamente testado buscando encontrar um algoritmo e configurações que fossem pouco suscetíveis à variações de luminosidade (pela presença de nuvens ou pelo passar das horas do dia). Nesse quesito foram avaliados diversos algoritmos consagrados de BGS: diferenças entre quadros e \textit{mixture of gaussians} conforme proposto por \citen{Zivkovic2004}. Sendo que, para o cenário em questão a última se mostrou mais eficiente, porém com uma alta carga computacional, principalmente quando executado no \textit{hardware} destino, tornando o desempenho inaceitável para o projeto em questão, fazendo que o mesmo fosse relegado.


\section{Segunda Implementação}

Uma vez avaliada a inviabilidade técnica da proposta inicial, foi feita uma nova análise do problema, uma vez que a utilização padrão do sistema é bastante simples: identificar a presença de um automóvel em uma determinada região de pixeis em um determinado instante de tempo com a maior confiabilidade possível. Apesar da especificação ser consideravelmente simples, sua aplicação depende de um processamento prévio, ou que sejam assumidas premissas que reduziriam a genericidade da solução. 
Em razão disso, se segmentou o problema em duas partes distintas: identificação do estado inicial do estacionamento e a alteração dos estados das vagas. Essa divisão se fez necessária devido ao fato de que em um caso real, dificilmente encontraremos um estacionamento momentaneamente vazio, então é preciso identificar qual a situação de cada vaga ao inicializar o sistema, além de que o conhecimento do estado anterior da vaga pode vir a ser útil ao se definir o novo estado.

Para identificar o estado inicial do estacionamento, foi verificada a solução proposta por \citen{True2007}, que desenvolveu um sistema de detecção de ocupação de vagas de estacionamento por meio de imagens estáticas. Nessa solução a identificação da ocupação das vagas de estacionamento é feita basicamente por meio da comparação de histograma dos canais de crominância (a* e b*) do espaço de cores CIELAB entre a região de interesse que é delimitada por uma vaga e um espaço sabidamente desocupado. Nessa proposta, True utilizou mecanismos de aprendizado de máquina para aumentar a confiabilidade dos resultados, além de detecção de \textit{features} nessas regiões.

\subsection{Preparação do sistema}

\subsection{Detecção de ocupação de vagas por análise estática}

\chapter{Resultados}

\chapter{Conclusões}

\chapter{Trabalhos Futuros}

\nocite{Sobral2014}
\nocite{6264659}
\nocite{KaewTraKulPong2002}
\nocite{BinZabawi2013}
\nocite{Lin2006}
\bibliographystyle{abntex2-alf}
\bibliography{export}

\annex
\chapter{Autorização para obtenção de imagens}
\label{autoriz}
\begin{figure}[h]
	\caption{Reprodução da autorização de captura de imagens da PUCRS}
	\centering
	\includegraphics[width=1.0\textwidth]{../../../Dropbox/__TG1/_images/autor}
	\label{fig:autor}
\end{figure}


\chapter{Trabalho de Graduação 1}
\label{tg1}

\end{document}
